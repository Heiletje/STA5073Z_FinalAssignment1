{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Prediction of Presidents\"\n",
        "title-size: small\n",
        "format: html\n",
        "execute:\n",
        "  echo: false\n",
        "  cache: true\n",
        "  freeze: auto\n",
        "---"
      ],
      "id": "7e9b93ea"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# preliminaries: load relevant libraries; import data; define colour palette\n",
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# load libraries\n",
        "import brewer2mpl\n",
        "import pandas as pd\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import numpy as np\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from tensorflow.keras.layers import LSTM, SpatialDropout1D\n",
        "\n",
        "# set seeds for reproducibility purposes\n",
        "np.random.seed(5)\n",
        "tf.random.set_seed(5)\n",
        "random.seed(5)\n",
        "os.environ['PYTHONHASHSEED'] = str(5)\n",
        "\n",
        "# import data \n",
        "data = pd.read_csv(\"sona.csv\")\n",
        "\n",
        "# Generate the RdGy colour palette\n",
        "# num_colors = 11\n",
        "# rdgy_palette = brewer2mpl.get_map('RdGy', 'Sequential', num_colors, reverse=True).mpl_colors\n",
        "\n",
        "#---------------------------------------------------------------------------------------------------------------------------"
      ],
      "id": "5d8685cf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# data pre-processing: prepare data  ~ subsettting by presidents, cleaning, and segmenting speeches into sentences\n",
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "# Assuming `data` is your original DataFrame\n",
        "# Select subset of four out of six presidents\n",
        "subset = data[data['president'].isin(['Mandela', 'Mbeki', 'Zuma', 'Ramaphosa'])]\n",
        "\n",
        "# Initialize a list to store the sentences\n",
        "sentences_data = []\n",
        "\n",
        "# Iterate through each row in the subset\n",
        "for index, row in subset.iterrows():\n",
        "    # Split the speech into sentences\n",
        "    speech_sentences = sent_tokenize(row['speech'])\n",
        "    \n",
        "    # For each sentence, create a new row with the same information\n",
        "    for sentence in speech_sentences:\n",
        "        sentences_data.append({\n",
        "            'sentence': sentence,\n",
        "            'year': row['year'],\n",
        "            'president': row['president'],\n",
        "            'date': row['date']\n",
        "        })\n",
        "\n",
        "# Create a new DataFrame with sentences\n",
        "sona_sentences = pd.DataFrame(sentences_data)\n",
        "\n",
        "# Filtering function to remove stop words and only words with a length of three characters or more\n",
        "english_words = set(nltk.corpus.words.words())\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def filter_text(text):\n",
        "    return ' '.join([word for word in text.split() if word not in stop_words and len(word) > 3 and word in english_words])\n",
        "\n",
        "# Apply the filter function to the cleaned sentences\n",
        "sona_sentences['cleaned_sentence'] = sona_sentences['sentence'].apply(filter_text)\n",
        "\n",
        "# Clean sentences\n",
        "sona_sentences['cleaned_sentence'] = sona_sentences['cleaned_sentence'].apply(lambda text: re.sub(r'[^A-Za-z\\s]', '', text).lower())\n",
        "\n",
        "# Output the number of rows\n",
        "sona_sentences.shape\n",
        "\n",
        "#---------------------------------------------------------------------------------------------------------------------------"
      ],
      "id": "12e78f89",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sona_sentences.head()"
      ],
      "id": "2eb046d7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# data pre-processing: create three different data structures for analysis\n",
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# create BoW ~ using top 150 words\n",
        "bow_vectorizer = CountVectorizer(max_features=150)\n",
        "bow_features = bow_vectorizer.fit_transform(sona_sentences['cleaned_sentence']).toarray()\n",
        "bow_features.shape\n",
        "# create tf-idf ~ using 150 words \n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=150)\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(sona_sentences['cleaned_sentence']).toarray()\n",
        "\n",
        "# create embeddings\n",
        "tokenized_speeches = [text.split() for text in sona_sentences['cleaned_sentence']]\n",
        "model = Word2Vec(sentences=tokenized_speeches, vector_size=150, window=5, min_count=1, workers=4)\n",
        "embeddings_features = np.array([np.mean([model.wv[word] for word in text.split() if word in model.wv] or [np.zeros(150)], axis=0) for text in sona_sentences['cleaned_sentence']])\n",
        "#---------------------------------------------------------------------------------------------------------------------------"
      ],
      "id": "efa3827d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# create splits for the data ~ 60-20-20 = training-validation-test\n",
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "seed = 5\n",
        "\n",
        "# create data split for BoW approach\n",
        "labels = sona_sentences['president']\n",
        "X_train_bow, X_temp_bow, y_train, y_temp = train_test_split(bow_features, labels, test_size=0.4, random_state=seed, stratify=labels)\n",
        "X_val_bow, X_test_bow, y_val, y_test = train_test_split(X_temp_bow, y_temp, test_size=0.5, random_state=seed, stratify=y_temp)\n",
        "\n",
        "# create data split for tf-idf approach\n",
        "X_train_tfidf, X_temp_tfidf = train_test_split(tfidf_features, test_size=0.4, random_state=seed, stratify=labels)\n",
        "X_val_tfidf, X_test_tfidf = train_test_split(X_temp_tfidf, test_size=0.5, random_state=seed, stratify=y_temp)\n",
        "\n",
        "# create data split for embedding approach\n",
        "X_train_emb, X_temp_emb = train_test_split(embeddings_features, test_size=0.4, random_state=seed, stratify=labels)\n",
        "X_val_emb, X_test_emb = train_test_split(X_temp_emb, test_size=0.5, random_state=seed, stratify=y_temp)\n",
        "\n",
        "\n",
        "#---------------------------------------------------------------------------------------------------------------------------"
      ],
      "id": "3b0fd463",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# Classification tree  ~ for BoW approach\n",
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "tree_params = {'max_depth': [3, 5, 10], 'min_samples_split': [2, 5, 10]}\n",
        "# hyperparameter gridsearch \n",
        "tree_clf_bow = GridSearchCV(DecisionTreeClassifier(), tree_params, cv=5) \n",
        "tree_clf_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "# predictions and evaluation\n",
        "y_pred_tree_bow = tree_clf_bow.predict(X_test_bow)\n",
        "print(\"Best parameters:\", tree_clf_bow.best_params_)\n",
        "print(\"\\nClassification Report (Classification Tree - BoW):\")\n",
        "print(classification_report(y_test, y_pred_tree_bow))\n",
        "\n",
        "# confusion matrix\n",
        "conf_matrix_tree_bow = confusion_matrix(y_test, y_pred_tree_bow)\n",
        "sns.heatmap(conf_matrix_tree_bow, annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "#---------------------------------------------------------------------------------------------------------------------------"
      ],
      "id": "9ba9898f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# Classification tree  ~ for tf-idf approach\n",
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "tree_params = {'max_depth': [3, 5, 10], 'min_samples_split': [2, 5, 10]}\n",
        "# hyperparameter gridsearch \n",
        "tree_clf_tfidf = GridSearchCV(DecisionTreeClassifier(), tree_params, cv=5) \n",
        "tree_clf_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# predictions and evaluation\n",
        "y_pred_tree_tfidf = tree_clf_tfidf.predict(X_test_tfidf)\n",
        "print(\"Best parameters:\", tree_clf_tfidf.best_params_)\n",
        "print(\"\\nClassification Report (Classification Tree - tf-idf):\")\n",
        "print(classification_report(y_test, y_pred_tree_tfidf))\n",
        "\n",
        "# confusion matrix\n",
        "conf_matrix_tree_tfidf = confusion_matrix(y_test, y_pred_tree_tfidf)\n",
        "sns.heatmap(conf_matrix_tree_tfidf, annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "#---------------------------------------------------------------------------------------------------------------------------"
      ],
      "id": "b147ccc4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# Classification tree  ~ for embedding approach\n",
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "tree_params = {'max_depth': [3, 5, 10], 'min_samples_split': [2, 5, 10]}\n",
        "# hyperparameter gridsearch \n",
        "tree_clf_emb = GridSearchCV(DecisionTreeClassifier(), tree_params, cv=5) \n",
        "tree_clf_emb.fit(X_train_emb, y_train)\n",
        "\n",
        "# predictions and evaluation\n",
        "y_pred_tree_emb = tree_clf_emb.predict(X_test_emb)\n",
        "print(\"Best parameters:\", tree_clf_emb.best_params_)\n",
        "print(\"\\nClassification Report (Classification Tree - embedding):\")\n",
        "print(classification_report(y_test, y_pred_tree_tfidf))\n",
        "\n",
        "# confusion matrix\n",
        "conf_matrix_tree_emb = confusion_matrix(y_test, y_pred_tree_emb)\n",
        "sns.heatmap(conf_matrix_tree_emb, annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "#---------------------------------------------------------------------------------------------------------------------------"
      ],
      "id": "4a216292",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# Feed-forward neural network ~ for BoW approach\n",
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "fnn_params = {'hidden_layer_sizes': [(100,), (100, 50)], 'activation': ['relu', 'tanh']}\n",
        "# hyperparameter gridsearch \n",
        "fnn_clf_bow = GridSearchCV(MLPClassifier(max_iter=1000), fnn_params, cv=5)\n",
        "fnn_clf_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "# Predictions and Evaluation\n",
        "y_pred_fnn_bow = fnn_clf_bow.predict(X_test_tfidf)\n",
        "print(\"Best parameters:\", fnn_clf_bow.best_params_)\n",
        "print(\"\\nClassification Report (Feed-forward NN - BoW):\")\n",
        "print(classification_report(y_test, y_pred_fnn_bow))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix_fnn_bow = confusion_matrix(y_test, y_pred_fnn_bow)\n",
        "sns.heatmap(conf_matrix_fnn_bow, annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "#---------------------------------------------------------------------------------------------------------------------------"
      ],
      "id": "4862f39b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# Feed-forward neural network ~ for tf-idf approach\n",
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "fnn_params = {'hidden_layer_sizes': [(100,), (100, 50)], 'activation': ['relu', 'tanh']}\n",
        "# hyperparameter gridsearch \n",
        "fnn_clf_tfidf = GridSearchCV(MLPClassifier(max_iter=1000), fnn_params, cv=5)\n",
        "fnn_clf_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predictions and Evaluation\n",
        "y_pred_fnn_tfidf = fnn_clf_tfidf.predict(X_test_tfidf)\n",
        "print(\"Best parameters:\", fnn_clf_tfidf.best_params_)\n",
        "print(\"\\nClassification Report (Feed-forward NN - tf-idf):\")\n",
        "print(classification_report(y_test, y_pred_fnn_tfidf))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix_fnn_tfidf = confusion_matrix(y_test, y_pred_fnn_tfidf)\n",
        "sns.heatmap(conf_matrix_fnn_tfidf, annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "#---------------------------------------------------------------------------------------------------------------------------"
      ],
      "id": "0822ef60",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# Feed-forward neural network ~ for embedding approach\n",
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "fnn_params = {'hidden_layer_sizes': [(100,), (100, 50)], 'activation': ['relu', 'tanh']}\n",
        "# hyperparameter gridsearch \n",
        "fnn_clf_emb = GridSearchCV(MLPClassifier(max_iter=1000), fnn_params, cv=5)\n",
        "fnn_clf_emb.fit(X_train_emb, y_train)\n",
        "\n",
        "# Predictions and Evaluation\n",
        "y_pred_fnn_emb = fnn_clf_emb.predict(X_test_emb)\n",
        "print(\"Best parameters:\", fnn_clf_emb.best_params_)\n",
        "print(\"\\nClassification Report (Feed-forward NN - embedding):\")\n",
        "print(classification_report(y_test, y_pred_fnn_emb))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix_fnn_emb = confusion_matrix(y_test, y_pred_fnn_emb)\n",
        "sns.heatmap(conf_matrix_fnn_emb, annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "#---------------------------------------------------------------------------------------------------------------------------"
      ],
      "id": "c938642b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# Convolutional neural network ~ for BoW approach\n",
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# prepare features \n",
        "y_train_nn = to_categorical(pd.factorize(y_train)[0])\n",
        "y_val_nn = to_categorical(pd.factorize(y_val)[0])\n",
        "y_test_nn = to_categorical(pd.factorize(y_test)[0])\n",
        "\n",
        "# define CNN \n",
        "def build_cnn_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=150, output_dim=hp.Int('embedding_output', min_value=32, max_value=128, step=32), input_length=150))\n",
        "    model.add(Conv1D(filters=hp.Int('filters', min_value=64, max_value=256, step=64), kernel_size=hp.Choice('kernel_size', values=[3, 5, 7]), activation='relu'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dropout(rate=hp.Float('dropout', min_value=0, max_value=0.5, step=0.1)))\n",
        "    model.add(Dense(units=len(y_train_nn[0]), activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# CNN tuner\n",
        "cnn_tuner_bow = RandomSearch(build_cnn_model, objective='val_accuracy', max_trials=5, executions_per_trial=3, directory='cnn_tuner', project_name='CNNHyperparamTuning')\n",
        "cnn_tuner_bow.search(X_train_bow, y_train_nn, epochs=10, validation_data=(X_val_bow, y_val_nn))\n",
        "\n",
        "# fit best CNN model \n",
        "best_hps_cnn_bow = cnn_tuner_bow.get_best_hyperparameters(num_trials=1)[0]\n",
        "best_cnn_model_bow = build_cnn_model(best_hps_cnn_bow)\n",
        "history_cnn_bow = best_cnn_model_bow.fit(X_train_bow, y_train_nn, epochs=10, validation_data=(X_val_bow, y_val_nn))\n",
        "\n",
        "# predictions and evaluation\n",
        "y_pred_cnn_bow = best_cnn_model_bow.predict(X_test_bow)\n",
        "y_pred_cnn_labels_bow = np.argmax(y_pred_cnn_bow, axis=1)\n",
        "y_test_labels_bow = np.argmax(y_test_nn, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report (CNN - BoW):\")\n",
        "print(classification_report(y_test_labels_bow, y_pred_cnn_labels_bow))\n",
        "\n",
        "# confusion matrix\n",
        "conf_matrix_cnn_bow = confusion_matrix(y_test_labels_bow, y_pred_cnn_labels_bow)\n",
        "sns.heatmap(conf_matrix_cnn_bow, annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# plotting accuracy curves\n",
        "plt.plot(history_cnn_bow.history['accuracy'])\n",
        "plt.plot(history_cnn_bow.history['val_accuracy'])\n",
        "plt.title('CNN Model Accuracy - BoW')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "#---------------------------------------------------------------------------------------------------------------------------"
      ],
      "id": "f6dabe1c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# Convolutional neural network ~ for tf-idf approach\n",
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# prepare features \n",
        "y_train_nn = to_categorical(pd.factorize(y_train)[0])\n",
        "y_val_nn = to_categorical(pd.factorize(y_val)[0])\n",
        "y_test_nn = to_categorical(pd.factorize(y_test)[0])\n",
        "\n",
        "# define CNN \n",
        "def build_cnn_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=150, output_dim=hp.Int('embedding_output', min_value=32, max_value=128, step=32), input_length=150))\n",
        "    model.add(Conv1D(filters=hp.Int('filters', min_value=64, max_value=256, step=64), kernel_size=hp.Choice('kernel_size', values=[3, 5, 7]), activation='relu'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dropout(rate=hp.Float('dropout', min_value=0, max_value=0.5, step=0.1)))\n",
        "    model.add(Dense(units=len(y_train_nn[0]), activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# CNN tuner\n",
        "cnn_tuner_tfidf = RandomSearch(build_cnn_model, objective='val_accuracy', max_trials=5, executions_per_trial=3, directory='cnn_tuner', project_name='CNNHyperparamTuning')\n",
        "cnn_tuner_tfidf.search(X_train_tfidf, y_train_nn, epochs=10, validation_data=(X_val_tfidf, y_val_nn))\n",
        "\n",
        "# fit best CNN model \n",
        "best_hps_cnn_tfidf = cnn_tuner_tfidf.get_best_hyperparameters(num_trials=1)[0]\n",
        "best_cnn_model_tfidf = build_cnn_model(best_hps_cnn_tfidf)\n",
        "history_cnn_tfidf = best_cnn_model_tfidf.fit(X_train_tfidf, y_train_nn, epochs=10, validation_data=(X_val_tfidf, y_val_nn))\n",
        "\n",
        "# predictions and evaluation\n",
        "y_pred_cnn_tfidf = best_cnn_model_tfidf.predict(X_test_tfidf)\n",
        "y_pred_cnn_labels_tfidf = np.argmax(y_pred_cnn_tfidf, axis=1)\n",
        "y_test_labels_tfidf = np.argmax(y_test_nn, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report (CNN - tf-idf):\")\n",
        "print(classification_report(y_test_labels_tfidf, y_pred_cnn_labels_tfidf))\n",
        "\n",
        "# confusion matrix\n",
        "conf_matrix_cnn_tfidf = confusion_matrix(y_test_labels_tfidf, y_pred_cnn_labels_tfidf)\n",
        "sns.heatmap(conf_matrix_cnn_tfidf, annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# plotting accuracy curves\n",
        "plt.plot(history_cnn_tfidf.history['accuracy'])\n",
        "plt.plot(history_cnn_tfidf.history['val_accuracy'])\n",
        "plt.title('CNN Model Accuracy - tf-idf')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "#---------------------------------------------------------------------------------------------------------------------------"
      ],
      "id": "ae274f70",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# Convolutional neural network ~ for embedding approach\n",
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# prepare features \n",
        "y_train_nn = to_categorical(pd.factorize(y_train)[0])\n",
        "y_val_nn = to_categorical(pd.factorize(y_val)[0])\n",
        "y_test_nn = to_categorical(pd.factorize(y_test)[0])\n",
        "\n",
        "# define CNN \n",
        "def build_cnn_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=150, output_dim=hp.Int('embedding_output', min_value=32, max_value=128, step=32), input_length=150))\n",
        "    model.add(Conv1D(filters=hp.Int('filters', min_value=64, max_value=256, step=64), kernel_size=hp.Choice('kernel_size', values=[3, 5, 7]), activation='relu'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dropout(rate=hp.Float('dropout', min_value=0, max_value=0.5, step=0.1)))\n",
        "    model.add(Dense(units=len(y_train_nn[0]), activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# CNN tuner\n",
        "cnn_tuner_emb = RandomSearch(build_cnn_model, objective='val_accuracy', max_trials=10, executions_per_trial=5, directory='cnn_tuner', project_name='CNNHyperparamTuning')\n",
        "cnn_tuner_emb.search(X_train_emb, y_train_nn, epochs=10, validation_data=(X_val_emb, y_val_nn))\n",
        "\n",
        "# fit best CNN model \n",
        "best_hps_cnn_emb = cnn_tuner_emb.get_best_hyperparameters(num_trials=1)[0]\n",
        "best_cnn_model_emb = build_cnn_model(best_hps_cnn_emb)\n",
        "history_cnn_emb = best_cnn_model_emb.fit(X_train_emb, y_train_nn, epochs=10, validation_data=(X_val_emb, y_val_nn))\n",
        "\n",
        "# predictions and evaluation\n",
        "y_pred_cnn_emb = best_cnn_model_emb.predict(X_test_emb)\n",
        "y_pred_cnn_labels_emb = np.argmax(y_pred_cnn_emb, axis=1)\n",
        "y_test_labels_emb = np.argmax(y_test_nn, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report (CNN - embedding):\")\n",
        "print(classification_report(y_test_labels_emb, y_pred_cnn_labels_emb))\n",
        "\n",
        "# confusion matrix\n",
        "conf_matrix_cnn_emb = confusion_matrix(y_test_labels_emb, y_pred_cnn_labels_emb)\n",
        "sns.heatmap(conf_matrix_cnn_emb, annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# plotting accuracy curves\n",
        "plt.plot(history_cnn_emb.history['accuracy'])\n",
        "plt.plot(history_cnn_emb.history['val_accuracy'])\n",
        "plt.title('CNN Model Accuracy - embedding')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "id": "484221c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# Recurrent neural network ~ for BoW approach\n",
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# prepare features \n",
        "y_train_nn = to_categorical(pd.factorize(y_train)[0])\n",
        "y_val_nn = to_categorical(pd.factorize(y_val)[0])\n",
        "y_test_nn = to_categorical(pd.factorize(y_test)[0])\n",
        "\n",
        "def build_rnn_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=150, output_dim=hp.Int('embedding_output', min_value=32, max_value=128, step=32), input_length=150))\n",
        "    model.add(SpatialDropout1D(hp.Float('spatial_dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
        "    model.add(LSTM(units=hp.Int('lstm_units', min_value=50, max_value=150, step=25), dropout=hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1), recurrent_dropout=hp.Float('recurrent_dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
        "    model.add(Dense(units=len(y_train_nn[0]), activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# RNN Tuner\n",
        "rnn_tuner_bow = RandomSearch(build_rnn_model, objective='val_accuracy', max_trials=5, executions_per_trial=3, directory='rnn_tuner', project_name='RNNHyperparamTuning')\n",
        "rnn_tuner_bow.search(X_train_bow, y_train_nn, epochs=10, validation_data=(X_val_bow, y_val_nn))\n",
        "\n",
        "# Fit the best model\n",
        "best_hps_rnn_bow = rnn_tuner_bow.get_best_hyperparameters(num_trials=1)[0]\n",
        "best_rnn_model_bow = build_rnn_model(best_hps_rnn_bow)\n",
        "history_rnn_bow = best_rnn_model_bow.fit(X_train_bow, y_train_nn, epochs=10, validation_data=(X_val_bow, y_val_nn))\n",
        "\n",
        "# Predictions and Evaluation\n",
        "y_pred_rnn_bow = best_rnn_model_bow.predict(X_test_bow)\n",
        "y_pred_rnn_labels_bow = np.argmax(y_pred_rnn_bow, axis=1)\n",
        "y_test_label_bow = np.argmax(y_test_nn, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report (RNN - BoW):\")\n",
        "print(classification_report(y_test_label_bow, y_pred_rnn_labels_bow))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix_rnn_bow = confusion_matrix(y_test_label_bow, y_pred_rnn_labels_bow)\n",
        "sns.heatmap(conf_matrix_rnn_bow, annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# Plotting accuracy curves\n",
        "plt.plot(history_rnn_bow.history['accuracy'])\n",
        "plt.plot(history_rnn_bow.history['val_accuracy'])\n",
        "plt.title('RNN Model Accuracy - BoW')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "#---------------------------------------------------------------------------------------------------------------------------"
      ],
      "id": "360c15de",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# Recurrent neural network ~ for tf-idf approach\n",
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# prepare features \n",
        "y_train_nn = to_categorical(pd.factorize(y_train)[0])\n",
        "y_val_nn = to_categorical(pd.factorize(y_val)[0])\n",
        "y_test_nn = to_categorical(pd.factorize(y_test)[0])\n",
        "\n",
        "def build_rnn_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=150, output_dim=hp.Int('embedding_output', min_value=32, max_value=128, step=32), input_length=150))\n",
        "    model.add(SpatialDropout1D(hp.Float('spatial_dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
        "    model.add(LSTM(units=hp.Int('lstm_units', min_value=50, max_value=150, step=25), dropout=hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1), recurrent_dropout=hp.Float('recurrent_dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
        "    model.add(Dense(units=len(y_train_nn[0]), activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# RNN Tuner\n",
        "rnn_tuner_tfidf = RandomSearch(build_rnn_model, objective='val_accuracy', max_trials=5, executions_per_trial=3, directory='rnn_tuner', project_name='RNNHyperparamTuning')\n",
        "rnn_tuner_tfidf.search(X_train_tfidf, y_train_nn, epochs=10, validation_data=(X_val_tfidf, y_val_nn))\n",
        "\n",
        "# Fit the best model\n",
        "best_hps_rnn_tfidf = rnn_tuner_tfidf.get_best_hyperparameters(num_trials=1)[0]\n",
        "best_rnn_model_tfidf = build_rnn_model(best_hps_rnn_tfidf)\n",
        "history_rnn_tfidf = best_rnn_model_tfidf.fit(X_train_tfidf, y_train_nn, epochs=10, validation_data=(X_val_tfidf, y_val_nn))\n",
        "\n",
        "# Predictions and Evaluation\n",
        "y_pred_rnn_tfidf = best_rnn_model_tfidf.predict(X_test_tfidf)\n",
        "y_pred_rnn_labels_tfidf = np.argmax(y_pred_rnn_tfidf, axis=1)\n",
        "y_test_label_tfidf = np.argmax(y_test_nn, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report (RNN - tf-idf):\")\n",
        "print(classification_report(y_test_label_tfidf, y_pred_rnn_labels_tfidf))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix_rnn_tfidf = confusion_matrix(y_test_label_tfidf, y_pred_rnn_labels_tfidf)\n",
        "sns.heatmap(conf_matrix_rnn_tfidf, annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# Plotting accuracy curves\n",
        "plt.plot(history_rnn_tfidf.history['accuracy'])\n",
        "plt.plot(history_rnn_tfidf.history['val_accuracy'])\n",
        "plt.title('RNN Model Accuracy - tf-idf')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "#---------------------------------------------------------------------------------------------------------------------------"
      ],
      "id": "33840ca2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# Recurrent neural network ~ for embedding approach\n",
        "#---------------------------------------------------------------------------------------------------------------------------\n",
        "# prepare features \n",
        "y_train_nn = to_categorical(pd.factorize(y_train)[0])\n",
        "y_val_nn = to_categorical(pd.factorize(y_val)[0])\n",
        "y_test_nn = to_categorical(pd.factorize(y_test)[0])\n",
        "\n",
        "def build_rnn_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=150, output_dim=hp.Int('embedding_output', min_value=32, max_value=128, step=32), input_length=150))\n",
        "    model.add(SpatialDropout1D(hp.Float('spatial_dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
        "    model.add(LSTM(units=hp.Int('lstm_units', min_value=50, max_value=150, step=25), dropout=hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1), recurrent_dropout=hp.Float('recurrent_dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
        "    model.add(Dense(units=len(y_train_nn[0]), activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# RNN Tuner\n",
        "rnn_tuner_emb = RandomSearch(build_rnn_model, objective='val_accuracy', max_trials=5, executions_per_trial=3, directory='rnn_tuner', project_name='RNNHyperparamTuning')\n",
        "rnn_tuner_emb.search(X_train_emb, y_train_nn, epochs=10, validation_data=(X_val_emb, y_val_nn))\n",
        "\n",
        "# Fit the best model\n",
        "best_hps_rnn_emb = rnn_tuner_emb.get_best_hyperparameters(num_trials=1)[0]\n",
        "best_rnn_model_emb = build_rnn_model(best_hps_rnn_emb)\n",
        "history_rnn_emb = best_rnn_model_emb.fit(X_train_emb, y_train_nn, epochs=10, validation_data=(X_val_emb, y_val_nn))\n",
        "\n",
        "# Predictions and Evaluation\n",
        "y_pred_rnn_emb = best_rnn_model_emb.predict(X_test_emb)\n",
        "y_pred_rnn_labels_emb = np.argmax(y_pred_rnn_emb, axis=1)\n",
        "y_test_label_emb = np.argmax(y_test_nn, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report (RNN - embedding):\")\n",
        "print(classification_report(y_test_label_emb, y_pred_rnn_labels_emb))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix_rnn_emb = confusion_matrix(y_test_label_emb, y_pred_rnn_labels_emb)\n",
        "sns.heatmap(conf_matrix_rnn_emb, annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# Plotting accuracy curves\n",
        "plt.plot(history_rnn_emb.history['accuracy'])\n",
        "plt.plot(history_rnn_emb.history['val_accuracy'])\n",
        "plt.title('RNN Model Accuracy - embedding')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "#---------------------------------------------------------------------------------------------------------------------------"
      ],
      "id": "32948ade",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}