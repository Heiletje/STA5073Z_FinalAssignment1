{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4571f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import types\n",
    "import json\n",
    "\n",
    "# figure size/format\n",
    "fig_width = 7\n",
    "fig_height = 5\n",
    "fig_format = 'retina'\n",
    "fig_dpi = 96\n",
    "\n",
    "# matplotlib defaults / format\n",
    "try:\n",
    "  import matplotlib.pyplot as plt\n",
    "  plt.rcParams['figure.figsize'] = (fig_width, fig_height)\n",
    "  plt.rcParams['figure.dpi'] = fig_dpi\n",
    "  plt.rcParams['savefig.dpi'] = fig_dpi\n",
    "  from IPython.display import set_matplotlib_formats\n",
    "  set_matplotlib_formats(fig_format)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# plotly use connected mode\n",
    "try:\n",
    "  import plotly.io as pio\n",
    "  pio.renderers.default = \"notebook_connected\"\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "# enable pandas latex repr when targeting pdfs\n",
    "try:\n",
    "  import pandas as pd\n",
    "  if fig_format == 'pdf':\n",
    "    pd.set_option('display.latex.repr', True)\n",
    "except Exception:\n",
    "  pass\n",
    "\n",
    "\n",
    "\n",
    "# output kernel dependencies\n",
    "kernel_deps = dict()\n",
    "for module in list(sys.modules.values()):\n",
    "  # Some modules play games with sys.modules (e.g. email/__init__.py\n",
    "  # in the standard library), and occasionally this can cause strange\n",
    "  # failures in getattr.  Just ignore anything that's not an ordinary\n",
    "  # module.\n",
    "  if not isinstance(module, types.ModuleType):\n",
    "    continue\n",
    "  path = getattr(module, \"__file__\", None)\n",
    "  if not path:\n",
    "    continue\n",
    "  if path.endswith(\".pyc\") or path.endswith(\".pyo\"):\n",
    "    path = path[:-1]\n",
    "  if not os.path.exists(path):\n",
    "    continue\n",
    "  kernel_deps[path] = os.stat(path).st_mtime\n",
    "print(json.dumps(kernel_deps))\n",
    "\n",
    "# set run_path if requested\n",
    "if r'/Users/heiletjevanzyl/Desktop/DSFI/STA5073Z_FinalAssignment1':\n",
    "  os.chdir(r'/Users/heiletjevanzyl/Desktop/DSFI/STA5073Z_FinalAssignment1')\n",
    "\n",
    "# reset state\n",
    "%reset\n",
    "\n",
    "def ojs_define(**kwargs):\n",
    "  import json\n",
    "  try:\n",
    "    # IPython 7.14 preferred import\n",
    "    from IPython.display import display, HTML\n",
    "  except:\n",
    "    from IPython.core.display import display, HTML\n",
    "\n",
    "  # do some minor magic for convenience when handling pandas\n",
    "  # dataframes\n",
    "  def convert(v):\n",
    "    try:\n",
    "      import pandas as pd\n",
    "    except ModuleNotFoundError: # don't do the magic when pandas is not available\n",
    "      return v\n",
    "    if type(v) == pd.Series:\n",
    "      v = pd.DataFrame(v)\n",
    "    if type(v) == pd.DataFrame:\n",
    "      j = json.loads(v.T.to_json(orient='split'))\n",
    "      return dict((k,v) for (k,v) in zip(j[\"index\"], j[\"data\"]))\n",
    "    else:\n",
    "      return v\n",
    "  \n",
    "  v = dict(contents=list(dict(name=key, value=convert(value)) for (key, value) in kwargs.items()))\n",
    "  display(HTML('<script type=\"ojs-define\">' + json.dumps(v) + '</script>'), metadata=dict(ojs_define = True))\n",
    "globals()[\"ojs_define\"] = ojs_define\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bddde94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# preliminaries: load relevant libraries; import data; define colour palette\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# load libraries\n",
    "import brewer2mpl\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import joblib\n",
    "import numpy as np\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import History\n",
    "from tensorflow.keras.layers import LSTM, SpatialDropout1D\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tabulate import tabulate\n",
    "\n",
    "# set seeds for reproducibility purposes\n",
    "seed = 5\n",
    "np.random.seed(5)\n",
    "tf.random.set_seed(5)\n",
    "random.seed(5)\n",
    "os.environ['PYTHONHASHSEED'] = str(5)\n",
    "\n",
    "# import data \n",
    "data = pd.read_csv(\"sona_raw.csv\")\n",
    "\n",
    "# generate the RdGy colour palette\n",
    "num_colors = 11\n",
    "rdgy_palette = brewer2mpl.get_map('RdGy', 'Diverging', num_colors, reverse=True).mpl_colors\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db1b9f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# data pre-processing: prepare data  ~ subsettting by presidents, cleaning, and segmenting speeches into sentences\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# select subset of four out of six presidents\n",
    "subset = data[data['president'].isin(['Mandela', 'Mbeki', 'Zuma', 'Ramaphosa'])]\n",
    "\n",
    "# initialize a list to store the sentences\n",
    "sentences_data = []\n",
    "\n",
    "# iterate through each row in the subset\n",
    "for index, row in subset.iterrows():\n",
    "    # split the speech into sentences\n",
    "    speech_sentences = sent_tokenize(row['speech'])\n",
    "    \n",
    "    # for each sentence, create a new row with the same information\n",
    "    for sentence in speech_sentences:\n",
    "        sentences_data.append({\n",
    "            'sentence': sentence,\n",
    "            'year': row['year'],\n",
    "            'president': row['president'],\n",
    "            'date': row['date']\n",
    "        })\n",
    "\n",
    "# create a new dataframe with sentences\n",
    "sona_sentences = pd.DataFrame(sentences_data)\n",
    "\n",
    "# filtering function to remove stop words and only words with a length of three characters or more\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def filter_text(text):\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words and len(word) > 3 and word in english_words])\n",
    "\n",
    "# apply the filter function to the cleaned sentences\n",
    "sona_sentences['cleaned_sentence'] = sona_sentences['sentence'].apply(filter_text)\n",
    "\n",
    "# clean sentences\n",
    "sona_sentences['cleaned_sentence'] = sona_sentences['cleaned_sentence'].apply(lambda text: re.sub(r'[^A-Za-z\\s]', '', text).lower())\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c226f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# exploratory data analysis: plot sentence counts \n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62443106",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# data pre-processing: create three different data structures for analysis\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# create BoW ~ using top 150 words\n",
    "bow_vectorizer = CountVectorizer(max_features=150)\n",
    "bow_features = bow_vectorizer.fit_transform(sona_sentences['cleaned_sentence']).toarray()\n",
    "bow_features.shape\n",
    "# create tf-idf ~ using 150 words \n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=150)\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(sona_sentences['cleaned_sentence']).toarray()\n",
    "\n",
    "# create embeddings for other models\n",
    "tokenized_speeches = [text.split() for text in sona_sentences['cleaned_sentence']]\n",
    "model = Word2Vec(sentences=tokenized_speeches, vector_size=150, window=5, min_count=1, workers=4)\n",
    "embeddings_features = np.array([np.mean([model.wv[word] for word in text.split() if word in model.wv] or [np.zeros(150)], axis=0) for text in sona_sentences['cleaned_sentence']])\n",
    "\n",
    "def sentence_to_avg_vector(sentence, model):\n",
    "    return np.mean([model.wv[word] for word in sentence.split() if word in model.wv] or [np.zeros(model.vector_size)], axis=0)\n",
    "\n",
    "# Convert sentences to sequences of vectors\n",
    "vector_sequences = [[model.wv[word] for word in sentence.split() if word in model.wv] for sentence in sona_sentences['cleaned_sentence']]\n",
    "\n",
    "# Determine the length of the longest sequence\n",
    "max_seq_length = max(len(sequence) for sequence in vector_sequences)\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "padded_sequences = pad_sequences(vector_sequences, maxlen=max_seq_length, padding='post', dtype='float32', value=0)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fd79d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# create splits for the data ~ 60-20-20 = training-validation-test\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "seed = 5\n",
    "\n",
    "# create data split for BoW approach\n",
    "labels = sona_sentences['president']\n",
    "X_train_bow, X_temp_bow, y_train, y_temp = train_test_split(bow_features, labels, test_size=0.4, random_state=seed, stratify=labels)\n",
    "X_val_bow, X_test_bow, y_val, y_test = train_test_split(X_temp_bow, y_temp, test_size=0.5, random_state=seed, stratify=y_temp)\n",
    "\n",
    "# create data split for tf-idf approach\n",
    "X_train_tfidf, X_temp_tfidf = train_test_split(tfidf_features, test_size=0.4, random_state=seed, stratify=labels)\n",
    "X_val_tfidf, X_test_tfidf = train_test_split(X_temp_tfidf, test_size=0.5, random_state=seed, stratify=y_temp)\n",
    "\n",
    "# create data split for embedding approach\n",
    "X_train_emb, X_temp_emb = train_test_split(embeddings_features, test_size=0.4, random_state=seed, stratify=labels)\n",
    "X_val_emb, X_test_emb = train_test_split(X_temp_emb, test_size=0.5, random_state=seed, stratify=y_temp)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "X_train_rnnemb, X_temp_rnnemb, y_train, y_temp = train_test_split(padded_sequences, labels, test_size=0.4, random_state=seed, stratify=labels)\n",
    "X_val_rnnemb, X_test_rnnemb, y_val, y_test = train_test_split(X_temp_rnnemb, y_temp, test_size=0.5, random_state=seed, stratify=y_temp)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ffe937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# Classification tree ~ define general function for all approaches\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "def decision_tree_analysis(X_train, X_val, X_test, y_train, y_val, y_test, feature_set_name, results_dir='results'):\n",
    "    # Define file paths\n",
    "    model_path = os.path.join(results_dir, f'tree_clf_{feature_set_name}.pkl')\n",
    "    plot_path = os.path.join(results_dir, f'cv_results_{feature_set_name}.png')\n",
    "\n",
    "    # Check if results already exist\n",
    "    if os.path.exists(model_path):\n",
    "        tree_clf = joblib.load(model_path)\n",
    "    else:\n",
    "        # Hyperparameter grid search\n",
    "        tree_params = {'max_depth': [3, 5, 7], 'min_samples_split': [2, 5, 10]}\n",
    "        tree_clf = GridSearchCV(DecisionTreeClassifier(), tree_params, cv=5, n_jobs=-1)\n",
    "        tree_clf.fit(X_train, y_train)\n",
    "\n",
    "        # Save the trained model\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        joblib.dump(tree_clf, model_path)\n",
    "\n",
    "    # Predictions and evaluation on test set\n",
    "    y_pred_tree_test = tree_clf.predict(X_test)\n",
    "    print(f\"Best parameters ({feature_set_name}):\", tree_clf.best_params_)\n",
    "\n",
    "    # Generate and display formatted classification report\n",
    "    report = classification_report(y_test, y_pred_tree_test, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    formatted_report = format_classification_report(report_df)\n",
    "    print(f\"\\nClassification Report on Test Set (Classification Tree - {feature_set_name}):\")\n",
    "    print(formatted_report)\n",
    "\n",
    "    # Plot and display confusion matrix\n",
    "    conf_matrix_tree_test = confusion_matrix(y_test, y_pred_tree_test)\n",
    "    sns.heatmap(conf_matrix_tree_test, annot=True, fmt='g', cmap=rdgy_palette)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix on Test Set - {feature_set_name}')\n",
    "    plt.show()\n",
    "\n",
    "    # Extract, plot, and save CV results heatmap\n",
    "    results = tree_clf.cv_results_\n",
    "    df = pd.DataFrame({\n",
    "        'max_depth': results['param_max_depth'],\n",
    "        'min_samples_split': results['param_min_samples_split'],\n",
    "        'mean_test_score': results['mean_test_score']\n",
    "    })\n",
    "    pivoted = df.pivot(index='max_depth', columns='min_samples_split', values='mean_test_score')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(pivoted, annot=True, fmt=\".4f\", cmap=rdgy_palette)\n",
    "    plt.title(f'Mean CV Score Heatmap - {feature_set_name}')\n",
    "    plt.xlabel('Min Samples Split')\n",
    "    plt.ylabel('Max Depth')\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()\n",
    "\n",
    "def format_classification_report(report_df):\n",
    "    # Format the DataFrame for better readability\n",
    "    report_df['precision'] = report_df['precision'].apply(lambda x: f\"{x:.2f}\")\n",
    "    report_df['recall'] = report_df['recall'].apply(lambda x: f\"{x:.2f}\")\n",
    "    report_df['f1-score'] = report_df['f1-score'].apply(lambda x: f\"{x:.2f}\")\n",
    "    report_df['support'] = report_df['support'].apply(lambda x: f\"{int(x)}\")\n",
    "\n",
    "    # Convert to a table format using tabulate\n",
    "    table = tabulate(report_df, headers='keys', tablefmt='psql', showindex=True)\n",
    "\n",
    "    return table\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08764938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# Classification tree  ~ for BoW approach\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "decision_tree_analysis(X_train_bow, X_val_bow, X_test_bow, y_train, y_val, y_test, 'bow')\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2490d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# Classification tree  ~ for tf-idf approach\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "decision_tree_analysis(X_train_tfidf, X_val_tfidf, X_test_tfidf, y_train, y_val, y_test, 'tfidf')\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3059836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# Classification tree  ~ for embedding approach\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "decision_tree_analysis(X_train_emb, X_val_emb, X_test_emb, y_train, y_val, y_test, 'embedding')\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "658cbdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hidden_layer_sizes, activation, input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    for i, layer_size in enumerate(hidden_layer_sizes):\n",
    "        if i == 0:\n",
    "            model.add(Dense(layer_size, activation=activation, input_shape=input_shape))\n",
    "        else:\n",
    "            model.add(Dense(layer_size, activation=activation))\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # Output layer\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_model(X_train, X_val, X_test, y_train, y_val, y_test, hidden_layer_sizes, activation, feature_set_name, results_dir='model_results', epochs=100, batch_size=32):\n",
    "    # Encoding target variables\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_val_encoded = label_encoder.transform(y_val)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "    num_classes = len(np.unique(y_train_encoded))\n",
    "\n",
    "    # Define model and history paths\n",
    "    model_path = os.path.join(results_dir, f'model_{feature_set_name}_{hidden_layer_sizes}_{activation}.h5')\n",
    "    history_path = os.path.join(results_dir, f'history_{feature_set_name}_{hidden_layer_sizes}_{activation}.pkl')\n",
    "\n",
    "    if os.path.exists(model_path) and os.path.exists(history_path):\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        history = joblib.load(history_path)\n",
    "    else:\n",
    "        # Create, train, and save the model\n",
    "        model = create_model(hidden_layer_sizes, activation, (X_train.shape[1],), num_classes)\n",
    "        history = model.fit(X_train, y_train_encoded, validation_data=(X_val, y_val_encoded), epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        model.save(model_path)\n",
    "        joblib.dump(history.history, history_path)\n",
    "\n",
    "    # Extract loss values\n",
    "    loss = history['loss'] if isinstance(history, dict) else history.history['loss']\n",
    "    val_loss = history['val_loss'] if isinstance(history, dict) else history.history['val_loss']\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(loss, label='Train', linestyle='-', color='r')\n",
    "    plt.plot(val_loss, label='Validation', linestyle='--', color='gray')\n",
    "    plt.title(f'Model Loss - {feature_set_name} - {hidden_layer_sizes} - {activation}')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate the model and print classification report\n",
    "    y_pred_encoded = np.argmax(model.predict(X_test), axis=1)\n",
    "    y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    formatted_report = format_classification_report(report_df)\n",
    "    print(f\"\\nClassification Report ({feature_set_name} - {hidden_layer_sizes} - {activation}):\")\n",
    "    print(formatted_report)\n",
    "\n",
    "    # Plot and display confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='g', cmap=rdgy_palette)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'Confusion Matrix - {feature_set_name} - {hidden_layer_sizes} - {activation}')\n",
    "    plt.show()\n",
    "\n",
    "    return model, history\n",
    "\n",
    "def format_classification_report(report_df):\n",
    "    # Format the DataFrame for better readability\n",
    "    report_df['precision'] = report_df['precision'].apply(lambda x: f\"{x:.2f}\")\n",
    "    report_df['recall'] = report_df['recall'].apply(lambda x: f\"{x:.2f}\")\n",
    "    report_df['f1-score'] = report_df['f1-score'].apply(lambda x: f\"{x:.2f}\")\n",
    "    report_df['support'] = report_df['support'].apply(lambda x: f\"{int(x)}\")\n",
    "\n",
    "    # Convert to a table format using tabulate\n",
    "    table = tabulate(report_df, headers='keys', tablefmt='psql', showindex=True)\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f80dce3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# Feed-forward neural network ~ for BoW approach\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "fnn_params = [(100,), (100, 50)]\n",
    "activations = ['relu', 'tanh']\n",
    "\n",
    "for hidden_layers in fnn_params:\n",
    "    for activation in activations:\n",
    "        print(f\"\\nTraining model with {hidden_layers} hidden layers and {activation} activation\")\n",
    "        train_and_evaluate_model(X_train_bow, X_val_bow, X_test_bow, y_train, y_val, y_test, hidden_layers, activation, 'bow')\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e87b5860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# Feed-forward neural network ~ for tf-idf approach\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "fnn_params = [(100,), (100, 50)]\n",
    "activations = ['relu', 'tanh']\n",
    "\n",
    "for hidden_layers in fnn_params:\n",
    "    for activation in activations:\n",
    "        print(f\"\\nTraining model with {hidden_layers} hidden layers and {activation} activation\")\n",
    "        train_and_evaluate_model(X_train_tfidf, X_val_tfidf, X_test_tfidf, y_train, y_val, y_test, hidden_layers, activation, 'tf-idf')\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "661f6bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# Feed-forward neural network ~ for embedding approach\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "fnn_params = [(100,), (100, 50)]\n",
    "activations = ['relu', 'tanh']\n",
    "\n",
    "for hidden_layers in fnn_params:\n",
    "    for activation in activations:\n",
    "        print(f\"\\nTraining model with {hidden_layers} hidden layers and {activation} activation\")\n",
    "        train_and_evaluate_model(X_train_emb, X_val_emb, X_test_emb, y_train, y_val, y_test, hidden_layers, activation, 'embedding')\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b47ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape, num_classes, filters, kernel_size, dropout_rate):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_shape))  # Adjust to the shape of your features\n",
    "    model.add(Reshape((input_shape[0], 1)))  # Add a reshape layer to make it compatible with Conv1D\n",
    "    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu'))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # Output layer\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_cnn_model(X_train, X_val, X_test, y_train, y_val, y_test, filters, kernel_size, dropout_rate, feature_set_name, results_dir='model_results', epochs=100, batch_size=32):\n",
    "    # Encoding target variables\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "    y_val_encoded = label_encoder.transform(y_val)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "    num_classes = len(np.unique(y_train_encoded))\n",
    "\n",
    "    # Define model and history paths\n",
    "    model_path = os.path.join(results_dir, f'cnn_model_{feature_set_name}_{filters}_{kernel_size}_{dropout_rate}.h5')\n",
    "    history_path = os.path.join(results_dir, f'cnn_history_{feature_set_name}_{filters}_{kernel_size}_{dropout_rate}.pkl')\n",
    "\n",
    "    if os.path.exists(model_path) and os.path.exists(history_path):\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        history = joblib.load(history_path)\n",
    "    else:\n",
    "        # Create, train, and save the model\n",
    "        model = create_cnn_model((X_train.shape[1],), num_classes, filters, kernel_size, dropout_rate)\n",
    "        history = model.fit(X_train, y_train_encoded, validation_data=(X_val, y_val_encoded), epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        model.save(model_path)\n",
    "        joblib.dump(history.history, history_path)\n",
    "\n",
    "    # Extract loss values\n",
    "    loss = history['loss'] if isinstance(history, dict) else history.history['loss']\n",
    "    val_loss = history['val_loss'] if isinstance(history, dict) else history.history['val_loss']\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(loss, label='Train', linestyle='-', color='r')\n",
    "    plt.plot(val_loss, label='Validation', linestyle='--', color='gray')\n",
    "    plt.title(f'CNN Model Loss - {feature_set_name} - Filters:{filters} Kernel:{kernel_size} Dropout:{dropout_rate}')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate the model and print classification report\n",
    "    y_pred_encoded = np.argmax(model.predict(X_test), axis=1)\n",
    "    y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    formatted_report = format_classification_report(report_df)\n",
    "    print(f\"\\nClassification Report (CNN - {feature_set_name} - Filters:{filters} Kernel:{kernel_size} Dropout:{dropout_rate}):\")\n",
    "    print(formatted_report)\n",
    "\n",
    "    # Plot and display confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='g', cmap=rdgy_palette)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'CNN Confusion Matrix - {feature_set_name} - Filters:{filters} Kernel:{kernel_size} Dropout:{dropout_rate}')\n",
    "    plt.show()\n",
    "\n",
    "    return model, history\n",
    "\n",
    "def format_classification_report(report_df):\n",
    "    # Format the DataFrame for better readability\n",
    "    report_df['precision'] = report_df['precision'].apply(lambda x: f\"{x:.2f}\")\n",
    "    report_df['recall'] = report_df['recall'].apply(lambda x: f\"{x:.2f}\")\n",
    "    report_df['f1-score'] = report_df['f1-score'].apply(lambda x: f\"{x:.2f}\")\n",
    "    report_df['support'] = report_df['support'].apply(lambda x: f\"{int(x)}\")\n",
    "\n",
    "    # Convert to a table format using tabulate\n",
    "    table = tabulate(report_df, headers='keys', tablefmt='psql', showindex=True)\n",
    "\n",
    "    return table\n",
    "\n",
    "\n",
    "# Define a small set of hyperparameters for the search\n",
    "filters_options = [64, 128]  # Number of filters\n",
    "kernel_size_options = [3, 5]  # Size of the convolutional kernels\n",
    "dropout_rate_options = [0.2, 0.5]  # Dropout rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "671e4097",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# Convolutional neural network ~ for BoW approach\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Define other parameters\n",
    "feature_set_name = \"BoW\" \n",
    "epochs = 10  \n",
    "batch_size = 32 \n",
    "results_dir = \"cnn_bow_results\"\n",
    "\n",
    "# Prepare a list to store the results\n",
    "search_results = []\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for filters in filters_options:\n",
    "    for kernel_size in kernel_size_options:\n",
    "        for dropout_rate in dropout_rate_options:\n",
    "            print(f\"Training model with filters={filters}, kernel_size={kernel_size}, dropout_rate={dropout_rate}\")\n",
    "\n",
    "            # Train and evaluate the model\n",
    "            model, history = train_and_evaluate_cnn_model(X_train_bow, X_val_bow, X_test_bow, y_train, y_val, y_test, filters, kernel_size, dropout_rate, feature_set_name, results_dir, epochs, batch_size)\n",
    "\n",
    "            # Get the best validation accuracy from the history\n",
    "            if isinstance(history, dict):\n",
    "                best_val_accuracy = max(history['val_accuracy'])\n",
    "            else:  # If it's a History object\n",
    "                best_val_accuracy = max(history.history['val_accuracy'])\n",
    "\n",
    "            # Store the results\n",
    "            search_results.append({\n",
    "                'filters': filters,\n",
    "                'kernel_size': kernel_size,\n",
    "                'dropout_rate': dropout_rate,\n",
    "                'best_val_accuracy': best_val_accuracy\n",
    "            })\n",
    "\n",
    "# Sort the results by the best validation accuracy\n",
    "search_results = sorted(search_results, key=lambda x: x['best_val_accuracy'], reverse=True)\n",
    "\n",
    "# Prepare data for tabulation\n",
    "table_data = []\n",
    "for result in search_results:\n",
    "    table_data.append([result['filters'], result['kernel_size'], result['dropout_rate'], f\"{result['best_val_accuracy']:.4f}\"])\n",
    "\n",
    "# Print the formatted table\n",
    "print(tabulate(table_data, headers=['Filters', 'Kernel Size', 'Dropout Rate', 'Best Val Accuracy'], tablefmt='psql'))\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4a3e3923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# Convolutional neural network ~ for tf-idf approach\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# Define other parameters\n",
    "feature_set_name = \"tf-idf\" \n",
    "epochs = 10  \n",
    "batch_size = 32 \n",
    "results_dir = \"cnn_tfidf_results\"\n",
    "\n",
    "# Prepare a list to store the results\n",
    "search_results = []\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for filters in filters_options:\n",
    "    for kernel_size in kernel_size_options:\n",
    "        for dropout_rate in dropout_rate_options:\n",
    "            print(f\"Training model with filters={filters}, kernel_size={kernel_size}, dropout_rate={dropout_rate}\")\n",
    "\n",
    "            # Train and evaluate the model\n",
    "            model, history = train_and_evaluate_cnn_model(X_train_tfidf, X_val_tfidf, X_test_tfidf, y_train, y_val, y_test, filters, kernel_size, dropout_rate, feature_set_name, results_dir, epochs, batch_size)\n",
    "\n",
    "            # Get the best validation accuracy from the history\n",
    "            if isinstance(history, dict):\n",
    "                best_val_accuracy = max(history['val_accuracy'])\n",
    "            else:  # If it's a History object\n",
    "                best_val_accuracy = max(history.history['val_accuracy'])\n",
    "\n",
    "            # Store the results\n",
    "            search_results.append({\n",
    "                'filters': filters,\n",
    "                'kernel_size': kernel_size,\n",
    "                'dropout_rate': dropout_rate,\n",
    "                'best_val_accuracy': best_val_accuracy\n",
    "            })\n",
    "\n",
    "# Sort the results by the best validation accuracy\n",
    "search_results = sorted(search_results, key=lambda x: x['best_val_accuracy'], reverse=True)\n",
    "\n",
    "# Prepare data for tabulation\n",
    "table_data = []\n",
    "for result in search_results:\n",
    "    table_data.append([result['filters'], result['kernel_size'], result['dropout_rate'], f\"{result['best_val_accuracy']:.4f}\"])\n",
    "\n",
    "# Print the formatted table\n",
    "print(tabulate(table_data, headers=['Filters', 'Kernel Size', 'Dropout Rate', 'Best Val Accuracy'], tablefmt='psql'))\n",
    "#---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36a4f33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# Convolutional neural network ~ for embedding approach\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# Define other parameters\n",
    "feature_set_name = \"Embedding\" \n",
    "epochs = 10  \n",
    "batch_size = 32 \n",
    "results_dir = \"cnn_emb_results\"\n",
    "\n",
    "# Prepare a list to store the results\n",
    "search_results = []\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for filters in filters_options:\n",
    "    for kernel_size in kernel_size_options:\n",
    "        for dropout_rate in dropout_rate_options:\n",
    "            print(f\"Training model with filters={filters}, kernel_size={kernel_size}, dropout_rate={dropout_rate}\")\n",
    "\n",
    "            # Train and evaluate the model\n",
    "            model, history = train_and_evaluate_cnn_model(X_train_emb, X_val_emb, X_test_emb, y_train, y_val, y_test, filters, kernel_size, dropout_rate, feature_set_name, results_dir, epochs, batch_size)\n",
    "\n",
    "            # Get the best validation accuracy from the history\n",
    "            if isinstance(history, dict):\n",
    "                best_val_accuracy = max(history['val_accuracy'])\n",
    "            else:  # If it's a History object\n",
    "                best_val_accuracy = max(history.history['val_accuracy'])\n",
    "            \n",
    "\n",
    "            # Store the results\n",
    "            search_results.append({\n",
    "                'filters': filters,\n",
    "                'kernel_size': kernel_size,\n",
    "                'dropout_rate': dropout_rate,\n",
    "                'best_val_accuracy': best_val_accuracy\n",
    "            })\n",
    "\n",
    "# Sort the results by the best validation accuracy\n",
    "search_results = sorted(search_results, key=lambda x: x['best_val_accuracy'], reverse=True)\n",
    "\n",
    "# Prepare data for tabulation\n",
    "table_data = []\n",
    "for result in search_results:\n",
    "    table_data.append([result['filters'], result['kernel_size'], result['dropout_rate'], f\"{result['best_val_accuracy']:.4f}\"])\n",
    "\n",
    "# Print the formatted table\n",
    "print(tabulate(table_data, headers=['Filters', 'Kernel Size', 'Dropout Rate', 'Best Val Accuracy'], tablefmt='psql'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91894ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_model(input_shape, num_classes, lstm_units, spatial_dropout, dropout, recurrent_dropout):\n",
    "    model = Sequential()\n",
    "    # Reshape layer to convert 2D input to 3D\n",
    "    model.add(Reshape((input_shape[0], 1), input_shape=input_shape)) \n",
    "    model.add(SpatialDropout1D(spatial_dropout))\n",
    "    model.add(LSTM(units=lstm_units, dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "    model.add(Dense(units=num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def build_rnn_model2(num_classes, lstm_units, spatial_dropout, dropout, recurrent_dropout, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(SpatialDropout1D(spatial_dropout, input_shape=input_shape)) \n",
    "    model.add(LSTM(units=lstm_units, dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "    model.add(Dense(units=num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_rnn_model(X_train, X_val, X_test, y_train, y_val, y_test, lstm_units, spatial_dropout, dropout, recurrent_dropout, feature_set_name, results_dir='model_results', epochs=100, batch_size=32):\n",
    "    # Encoding target variables\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_train_encoded = to_categorical(label_encoder.fit_transform(y_train))\n",
    "    y_val_encoded = to_categorical(label_encoder.transform(y_val))\n",
    "    y_test_encoded = to_categorical(label_encoder.transform(y_test))\n",
    "\n",
    "    num_classes = y_train_encoded.shape[1]\n",
    "\n",
    "    # Define model and history paths\n",
    "    model_path = os.path.join(results_dir, f'rnn_model_{feature_set_name}_{lstm_units}_{spatial_dropout}_{dropout}_{recurrent_dropout}.h5')\n",
    "    history_path = os.path.join(results_dir, f'rnn_history_{feature_set_name}_{lstm_units}_{spatial_dropout}_{dropout}_{recurrent_dropout}.pkl')\n",
    "\n",
    "    if os.path.exists(model_path) and os.path.exists(history_path):\n",
    "        model = tf.keras.models.load_model(model_path)\n",
    "        history = joblib.load(history_path)\n",
    "    else:\n",
    "        # Create, train, and save the model\n",
    "        model = build_rnn_model((X_train.shape[1],), num_classes, lstm_units, spatial_dropout, dropout, recurrent_dropout)\n",
    "        history = model.fit(X_train, y_train_encoded, validation_data=(X_val, y_val_encoded), epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        model.save(model_path)\n",
    "        joblib.dump(history.history, history_path)\n",
    "\n",
    "    # Extract accuracy values\n",
    "    accuracy = history['accuracy'] if isinstance(history, dict) else history.history['accuracy']\n",
    "    val_accuracy = history['val_accuracy'] if isinstance(history, dict) else history.history['val_accuracy']\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(accuracy, label='Train', linestyle='-', color='r')\n",
    "    plt.plot(val_accuracy, label='Validation', linestyle='--', color='gray')\n",
    "    plt.title(f'RNN Model Accuracy - {feature_set_name} - LSTM:{lstm_units} SpatialDropout:{spatial_dropout} Dropout:{dropout} RecurrentDropout:{recurrent_dropout}')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate the model and print classification report\n",
    "    y_pred_encoded = np.argmax(model.predict(X_test), axis=1)\n",
    "    y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    formatted_report = format_classification_report(report_df)\n",
    "    print(f\"\\nClassification Report (RNN - {feature_set_name} - LSTM:{lstm_units} SpatialDropout:{spatial_dropout} Dropout:{dropout} RecurrentDropout:{recurrent_dropout}):\")\n",
    "    print(formatted_report)\n",
    "\n",
    "    # Plot and display confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='g', cmap=rdgy_palette)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'RNN Confusion Matrix - {feature_set_name} - LSTM:{lstm_units} SpatialDropout:{spatial_dropout} Dropout:{dropout} RecurrentDropout:{recurrent_dropout}')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return model, history\n",
    "\n",
    "def format_classification_report(report_df):\n",
    "    # Format the DataFrame for better readability\n",
    "    report_df['precision'] = report_df['precision'].apply(lambda x: f\"{x:.2f}\")\n",
    "    report_df['recall'] = report_df['recall'].apply(lambda x: f\"{x:.2f}\")\n",
    "    report_df['f1-score'] = report_df['f1-score'].apply(lambda x: f\"{x:.2f}\")\n",
    "    report_df['support'] = report_df['support'].apply(lambda x: f\"{int(x)}\")\n",
    "\n",
    "    # Convert to a table format using tabulate\n",
    "    table = tabulate(report_df, headers='keys', tablefmt='psql', showindex=True)\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96926b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# Recurrent neural network ~ for BoW approach\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Define other parameters\n",
    "feature_set_name = \"BoW\" \n",
    "epochs = 10  \n",
    "batch_size = 32 \n",
    "results_dir = \"rnn_bow_results\"\n",
    "\n",
    "# Hyperparameter options for RNN\n",
    "lstm_units_options = [50, 75]\n",
    "spatial_dropout_options = [0.0, 0.2]\n",
    "dropout_options = [0.2]\n",
    "recurrent_dropout_options = [0.2]\n",
    "\n",
    "# Prepare a list to store the results\n",
    "search_results = []\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for lstm_units in lstm_units_options:\n",
    "    for spatial_dropout in spatial_dropout_options:\n",
    "        for dropout in dropout_options:\n",
    "            for recurrent_dropout in recurrent_dropout_options:\n",
    "\n",
    "                # Train and evaluate the model\n",
    "                model, history = train_and_evaluate_rnn_model(X_train_bow, X_val_bow, X_test_bow, y_train, y_val, y_test, lstm_units, spatial_dropout, dropout, recurrent_dropout, feature_set_name, results_dir, epochs, batch_size)\n",
    "\n",
    "                # Get the best validation accuracy from the history\n",
    "                if isinstance(history, dict):\n",
    "                    best_val_accuracy = max(history['val_accuracy'])\n",
    "                else:  # If it's a History object\n",
    "                    best_val_accuracy = max(history.history['val_accuracy'])\n",
    "\n",
    "                # Store the results\n",
    "                search_results.append({\n",
    "                    'lstm_units': lstm_units,\n",
    "                    'spatial_dropout': spatial_dropout,\n",
    "                    'dropout': dropout,\n",
    "                    'recurrent_dropout': recurrent_dropout,\n",
    "                    'best_val_accuracy': best_val_accuracy\n",
    "                })\n",
    "\n",
    "# Sort the results by the best validation accuracy\n",
    "search_results = sorted(search_results, key=lambda x: x['best_val_accuracy'], reverse=True)\n",
    "\n",
    "# Prepare data for tabulation\n",
    "table_data = []\n",
    "for result in search_results:\n",
    "    table_data.append([\n",
    "        result['lstm_units'], \n",
    "        result['spatial_dropout'], \n",
    "        result['dropout'], \n",
    "        result['recurrent_dropout'], \n",
    "        f\"{result['best_val_accuracy']:.4f}\"\n",
    "    ])\n",
    "\n",
    "# Print the formatted table\n",
    "print(tabulate(table_data, headers=['LSTM Units', 'Spatial Dropout', 'Dropout', 'Recurrent Dropout', 'Best Val Accuracy'], tablefmt='psql'))\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f2c3fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# Recurrent neural network ~ for tf-idf approach\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# Define other parameters\n",
    "feature_set_name = \"tf-idf\" \n",
    "epochs = 10  \n",
    "batch_size = 32 \n",
    "results_dir = \"rnn_tfidf_results\"\n",
    "\n",
    "# Hyperparameter options for RNN\n",
    "lstm_units_options = [50, 75]\n",
    "spatial_dropout_options = [0.0, 0.2]\n",
    "dropout_options = [0.2]\n",
    "recurrent_dropout_options = [0.2]\n",
    "\n",
    "# Prepare a list to store the results\n",
    "search_results = []\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "\n",
    "for lstm_units in lstm_units_options:\n",
    "    for spatial_dropout in spatial_dropout_options:\n",
    "        for dropout in dropout_options:\n",
    "            for recurrent_dropout in recurrent_dropout_options:\n",
    "\n",
    "                # Train and evaluate the model\n",
    "                model, history = train_and_evaluate_rnn_model(X_train_tfidf, X_val_tfidf, X_test_tfidf, y_train, y_val, y_test, lstm_units, spatial_dropout, dropout, recurrent_dropout, feature_set_name, results_dir, epochs, batch_size)\n",
    "\n",
    "                # Get the best validation accuracy from the history\n",
    "                if isinstance(history, dict):\n",
    "                    best_val_accuracy = max(history['val_accuracy'])\n",
    "                else:  # If it's a History object\n",
    "                    best_val_accuracy = max(history.history['val_accuracy'])\n",
    "\n",
    "                # Store the results\n",
    "                search_results.append({\n",
    "                    'lstm_units': lstm_units,\n",
    "                    'spatial_dropout': spatial_dropout,\n",
    "                    'dropout': dropout,\n",
    "                    'recurrent_dropout': recurrent_dropout,\n",
    "                    'best_val_accuracy': best_val_accuracy\n",
    "                })\n",
    "\n",
    "# Sort the results by the best validation accuracy\n",
    "search_results = sorted(search_results, key=lambda x: x['best_val_accuracy'], reverse=True)\n",
    "\n",
    "# Prepare data for tabulation\n",
    "table_data = []\n",
    "for result in search_results:\n",
    "    table_data.append([\n",
    "        result['lstm_units'], \n",
    "        result['spatial_dropout'], \n",
    "        result['dropout'], \n",
    "        result['recurrent_dropout'], \n",
    "        f\"{result['best_val_accuracy']:.4f}\"\n",
    "    ])\n",
    "\n",
    "# Print the formatted table\n",
    "print(tabulate(table_data, headers=['LSTM Units', 'Spatial Dropout', 'Dropout', 'Recurrent Dropout', 'Best Val Accuracy'], tablefmt='psql'))\n",
    "#---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ea7aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# Recurrent neural network ~ for embedding approach\n",
    "#---------------------------------------------------------------------------------------------------------------------------\n",
    "# Define other parameters\n",
    "feature_set_name = \"Embedding\" \n",
    "epochs = 10  \n",
    "batch_size = 32 \n",
    "results_dir = \"rnn_emb_results\"\n",
    "\n",
    "# Hyperparameter options for RNN\n",
    "lstm_units_options = [50, 75]\n",
    "spatial_dropout_options = [0.0, 0.2]\n",
    "dropout_options = [0.2]\n",
    "recurrent_dropout_options = [0.2]\n",
    "\n",
    "# Prepare a list to store the results\n",
    "search_results = []\n",
    "\n",
    "# Iterate over all combinations of hyperparameters\n",
    "for lstm_units in lstm_units_options:\n",
    "    for spatial_dropout in spatial_dropout_options:\n",
    "        for dropout in dropout_options:\n",
    "            for recurrent_dropout in recurrent_dropout_options:\n",
    "\n",
    "                # Train and evaluate the model\n",
    "                model, history = train_and_evaluate_rnn_model(X_train_rnnemb, X_val_rnnemb, X_test_rnnemb, y_train, y_val, y_test, lstm_units, spatial_dropout, dropout, recurrent_dropout, feature_set_name, results_dir, epochs, batch_size)\n",
    "\n",
    "                # Get the best validation accuracy from the history\n",
    "                if isinstance(history, dict):\n",
    "                    best_val_accuracy = max(history['val_accuracy'])\n",
    "                else:  # If it's a History object\n",
    "                    best_val_accuracy = max(history.history['val_accuracy'])\n",
    "\n",
    "                # Store the results\n",
    "                search_results.append({\n",
    "                    'lstm_units': lstm_units,\n",
    "                    'spatial_dropout': spatial_dropout,\n",
    "                    'dropout': dropout,\n",
    "                    'recurrent_dropout': recurrent_dropout,\n",
    "                    'best_val_accuracy': best_val_accuracy\n",
    "                })\n",
    "\n",
    "# Sort the results by the best validation accuracy\n",
    "search_results = sorted(search_results, key=lambda x: x['best_val_accuracy'], reverse=True)\n",
    "\n",
    "# Prepare data for tabulation\n",
    "table_data = []\n",
    "for result in search_results:\n",
    "    table_data.append([\n",
    "        result['lstm_units'], \n",
    "        result['spatial_dropout'], \n",
    "        result['dropout'], \n",
    "        result['recurrent_dropout'], \n",
    "        f\"{result['best_val_accuracy']:.4f}\"\n",
    "    ])\n",
    "\n",
    "# Print the formatted table\n",
    "print(tabulate(table_data, headers=['LSTM Units', 'Spatial Dropout', 'Dropout', 'Recurrent Dropout', 'Best Val Accuracy'], tablefmt='psql'))\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}