---
title: "Prediction of Presidents"
title-size: small
format: html
execute:
  echo: false
  cache: true
  freeze: auto
---


<div style="text-align: justify"> 

<h4> Abstract </h4> 
<hr> 
Converging machine learning (ML) and natural language processing (NLP) techniques, a predictive text-analysis was performed on the State of the Nation Addresses (SONAs) delivered by South African presidents spanning the period from 1994 to 2023. Different types of neural networks (NNs) were employed for the purpose of predicting the president from their spoken sentences. 
Serving as a foundational-comparative model, a naive classification tree (CT) was first fitted followed by varying-in-complexity models from a simple feed-forward NN (FNN) to more sophisticated NNs, such as recurrent (RNN) and convolutional (CNN). For each predictive model, the involved text was transformed into features via three approaches, namely bag-of-words (BoW), term-frequency, inverse-document-frequency (tf-idf), and embeddings. A comparison, in terms of correct-classification capability, between the numerous NNs across the differently defined features was conducted with the objective of obtaining the optimal predictive model for the political speeches. 


<h4> Introduction </h4> 
<hr> 
The domains of machine learning (ML) and natural language processing (NLP) intersect at text-based analysis involving the underlying aim of prediction. In this instance, the text of interest to analyze are the annual State of the Nation Address (SONA) speeches (from 1994 to 2023), whilst the predictive objective is pointed at determining which one of the six different South African presidents (F.W. de Klerk, N.R. Mandela, T.M. Mbeki, K.P. Motlanthe, J.G. Zuma, and M.C. Ramaphosa) delivered some sentence contained therein. 

In order to achieve this predictive analysis, diverse ML models with differing degrees of complexity are applied. First, in order to form a comparative basis for performance, a naïve classification tree is fitted. This is then followed with the implementation of three types of neural networks (NNs),each with ascending degrees of sophistication, specifically a simple one (feed-forward) and then more complex ones (recurrent and convolutional).



<h4> Literature Review </h4> 
<hr> 

<b><u> SONA  </b></u>




<h4> Data </h4>
<hr>


<b><u> Tokenization </b></u>

<b><u> Pre-processing </b></u>


<h4> Methods </h4>
<hr>

<b><u> Classification Tree (CT) </b></u>
![Schematic representation of an exemplary CT.](CT.png){#fig-CT}

A classification tree (CT) [@Breiman1984] is a straightforward ML technique predominately applied in categorical prediction contexts. This simple model operates on the principle of segmenting some data into subsets, where these partitions are based on distinct values of individual features. It is naïve in its approach given it considers each feature in isolation in order to make classifying decisions, hence often overlooking complex feature interactions. 

When applied to text-based data, a CT makes classification decisions based on either the frequency (a BoW representation) or weighted frequency of words (a tf-idf representation) as the features. As illustrated in @fig-CT, each CT decision node (shown as a grey, rectangular block) represents a rule where some threshold on a particular word's frequency (or weighted version thereof) is defined. While less conventional, word embeddings can also be utilized instead. Though, the CT is often found to be mediocre in its ability to fully exploit the rich semantic and syntactic relationships captured in embeddings. Given that embeddings provide a dense representation of words in a high-dimensional space, the simplistic decision boundaries of a CT are sub-par in capturing the associated contextual nuances [@Charbuty2021]. 

CT hyperparameters comprise of the minimum number of samples required to split a node  as well as the maximum tree depth. Using a grid-search approach coupled with a *k*-fold cross-validation (CV) process, various combinations of these two hyperparameters over smaller sets of data (folds) are systematically assessed to find a configuration where a chosen performance metric is optimized. Here, such a metric is the average of evaluation scores (like accuracy) obtained across all folds for the hyperparameter set yielding the highest value thereof.

CTs are superior in terms of its simplistic interpretability and visualization, though this advantage is tainted with its proneness to overfitting (with deep trees) and inability to capture feature complexity.  

<b><u> Feed-forward Neural Network (NN) </b></u>
![Schematic representation of an exemplary FNN](FNN.png){#fig-FNN}

A feed-forward Neural Network (FNN) is a type of artificial NN wherein connections between the nodes (neurons) do not form a cycle (uni-directional in nature). As depicted in @fig-FNN, this architecture is characterized by its stratified structure,  comprising of an input layer (grey circles), one or more hidden layers (white circles), and an output layer (black circles). In each layer, the nodes apply weights and biases to inputs before passing through an activation function.

In text analysis, when using BoW or tf-idf representations, each word is represented as a feature (i.e., a unique input neuron). FNNs transform text-based data into numerical vectors where each dimension corresponds to a word frequency (or weighted version thereof). These vectors are efficiently processed, thereby enabling the network to learn patterns that are predictive of an outcome of interest (like predicting presidents from sentences). With  the employment of embeddings, unlike CTs, FNNs are able to leverage such dense word representations to capture subtler semantic patterns, potentially leading to more enhanced predictions. 

FNN hyperparameters involve the number of hidden layers, the number of neurons in each hidden layer, the type of activation function (e.g., ReLU or tanh), and the learning rate. To identify the optimal network architecture, a grid search over these tuneable parameters is executed. 

In contrast to CTs, FNNs possess a higher aptitude to deal with feature intricacies, whether there is high dimensionality or the presence of non-linear relationships. Despite this, FNNs are also still susceptible to overfitting (with many layers/nodes). Additionally, despite its ability to capture semantic information, this is hindered by its inherent incapacity to simultaneously encapsulate the sequential/temporal relationship prevalent in text. 


<b><u> Convolutional Neural Network (CNN) </b></u>
![Schematic representation of an exemplary CNN.](CNN.png){#fig-CNN}


Convolutional Neural Networks (CNNs), primarily applied within the field of image-processing, have also proven proficiency in processing natural language processing (NLP) tasks. CNNs are designed to adaptively learn spatial hierarchies of features from input data. In the context of text, this means the model is able to learn any recognizable patterns or sequences of words which carry significant semantic meaning.

In comparison to CTs and FNNS, CNNs are the most specialized to incorporate embeddings, exploiting the associated representation to capture contextual information. The convolutional layers (as shown in @fig-CNN) are able to detect local patterns (like phrases or commonly co-occurring words) within these embeddings. With this model, the BoW and tf-idf approach is not ordinarily applied, given that both these representations lack the inherent sequential nature of text. Thus, the ability of CNNs to capture local dependencies is limited. 

CNN hyperparameters consist of the number convolutional layers, the size and number of filters (kernels), the size of the pooling layers, and the dropout rate. To configure the optimal set of these tunable parameters, a grid search approach is applied. 

CNNs are able to capture local dependencies in text-based data, allowing for more apt short-length pattern recognition. However, when dealing with long-range dependencies (like with sentences), the performance of CNNs is dampened. Lastly, given the implicit shared-weight architecture of the model, the tendency to overfit is not as prevalent compared to CTs and FNNS. 


<b><u> Recurrent Neural Network (RNN) </b></u>
![Schematic representation of an exemplary RNN](RNN.png){#fig-RNN}

Recurrent Neural Networks (RNNs) are designed to handle sequential data. This model shares a similar architecture to FNNs, as represented in @fig-RNN, with an exception of two differences. FNNs are uni-directional models, whilst RNNs are bi-directional in nature. For the latter model, this means that information from some nodes’ outputs are able to flow in a backward direction (represented as red arrows), affecting subsequent input to the same node. Moreover, RNNs share the same weight parameter within each layer unlike FNNs where different weights apply across each node.

In the context of text, this means RNNs are well-suited for tasks where understanding the order and context of words is paramount. Unlike FNNs, RNNs utilize internal state (memory) to process sequences of inputs, therefore allowing information across different parts of the text to be maintained. 

Like with CNNs, RNNs are more effective when employing embeddings in comparison to BoW or tf-idf representations. These latter two approaches do not preserve the sequential order of words, which is the key feature RNNs endeavour to exploit. Through embeddings, the model is able to understand context such as comprehending how the meaning of a word is contingent on its position. This is particularly pertinent in text-based analysis involving sentence structures. 

RNN hyperparameters, which can be optimized to values allowing for the best predictive model performance via a grid-search approach,  include number of recurrent units and the size thereof, dropout rates, and types of recurrent layers. 

Conversely to CNNs, RNNs are more capable of dealing with long-range dependencies prevalent in text-based data, given it implicitly depends on the existence of some sort of sequential element.  Despite this, these models are rather difficult to train given exploding gradients (especially with long sequences like sentences), whilst also being expensive both in terms of 
intensity and time. 

<b><u> Model performance metrics </b></u>



```{python}
#---------------------------------------------------------------------------------------------------------------------------
# preliminaries: load relevant libraries; import data; define colour palette
#---------------------------------------------------------------------------------------------------------------------------
# load libraries
import brewer2mpl
import pandas as pd
from nltk.tokenize import sent_tokenize
from sklearn.model_selection import train_test_split
import re
import joblib
import numpy as np
import nltk
import tensorflow as tf
import random
import os
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import re
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from gensim.models import Word2Vec
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
from sklearn.neural_network import MLPClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.utils import to_categorical
from kerastuner.tuners import RandomSearch
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import History
from tensorflow.keras.layers import LSTM, SpatialDropout1D
import matplotlib.ticker as ticker
from mpl_toolkits.mplot3d import Axes3D
from tensorflow.keras.preprocessing.sequence import pad_sequences

# set global parameters
plt.rcParams['font.family'] = 'Andale Mono'
plt.rcParams['xtick.labelsize'] = 14
plt.rcParams['ytick.labelsize'] = 14
plt.rcParams['axes.labelsize'] = 16
plt.rcParams['legend.fontsize'] = 14

# set seeds for reproducibility purposes
np.random.seed(5)
tf.random.set_seed(5)
random.seed(5)
os.environ['PYTHONHASHSEED'] = str(5)

# import data 
data = pd.read_csv("sona.csv")

# generate the RdGy colour palette
num_colors = 10
rdgy_palette = brewer2mpl.get_map('RdGy', 'Diverging', num_colors, reverse=True).mpl_colors

#---------------------------------------------------------------------------------------------------------------------------
```


```{python}
#---------------------------------------------------------------------------------------------------------------------------
# data pre-processing: prepare data  ~ subsettting by presidents, cleaning, and segmenting speeches into sentences
#---------------------------------------------------------------------------------------------------------------------------
# select subset of four out of six presidents
subset = data[data['president'].isin(['Mandela', 'Mbeki', 'Zuma', 'Ramaphosa'])]

# initialize a list to store the sentences
sentences_data = []

# iterate through each row in the subset
for index, row in subset.iterrows():
    # split the speech into sentences
    speech_sentences = sent_tokenize(row['speech'])
    
    # for each sentence, create a new row with the same information
    for sentence in speech_sentences:
        sentences_data.append({
            'sentence': sentence,
            'year': row['year'],
            'president': row['president'],
            'date': row['date']
        })

# create a new dataframe with sentences
sona_sentences = pd.DataFrame(sentences_data)

# filtering function to remove stop words and only words with a length of three characters or more
english_words = set(nltk.corpus.words.words())
stop_words = set(stopwords.words('english'))

def filter_text(text):
    return ' '.join([word for word in text.split() if word not in stop_words and len(word) > 3 and word in english_words])

# apply the filter function to the cleaned sentences
sona_sentences['cleaned_sentence'] = sona_sentences['sentence'].apply(filter_text)

# clean sentences
sona_sentences['cleaned_sentence'] = sona_sentences['cleaned_sentence'].apply(lambda text: re.sub(r'[^A-Za-z\s]', '', text).lower())

#---------------------------------------------------------------------------------------------------------------------------
```


<h4> Exploratory Data Analysis </h4> 
<hr> 

Figure 1 


```{python}
#----------------------------------------------------------------------------------------------------------------------------
# exploratory data analysis: plot speech counts
#----------------------------------------------------------------------------------------------------------------------------
# # create dataframe with counts of speeches for each president
# president_num_speeches = data['president'].value_counts().reset_index()
# president_num_speeches.columns = ['president', 'num_speeches']

# # setting a specific order for the presidents
# ordered_presidents = ['Mandela', 'Mbeki', 'Zuma', 'Ramaphosa']
# president_num_speeches['president'] = pd.Categorical(president_num_speeches['president'], categories=ordered_presidents, ordered=True)

# # sort the DataFrame based on the defined order
# president_num_speeches.sort_values('president', inplace=True)

# # plot the sentence counts
# plt.bar(president_num_speeches['president'], president_num_speeches['num_speeches'], color=rdgy_palette[2])

# # Customizing the plot
# plt.xlabel("President", fontweight='bold', family='Andale Mono')
# plt.ylabel("Number of Speeches", fontweight='bold', family='Andale Mono')
# plt.xticks(rotation=55, horizontalalignment='right', fontweight='bold', family='Andale Mono')
# plt.yticks(fontweight='bold', family='Andale Mono')
# plt.title("", fontweight='bold', ha='center')

# # Additional theme customizations
# plt.grid(False)
# plt.tight_layout()
# plt.savefig(f'EDA/president_num_speeches.png', bbox_inches='tight')
# plt.close() 
#---------------------------------------------------------------------------------------------------------------------------
```


<!-- ![Number of SONA speeches for each president.](EDA/president_num_speeches.png){#fig-president_num_speeches} -->

@fig-president_num_speeches delineates the distribution of speeches across the different presidents, revealing a notable underrepresentation of de Klerk and Motlanthe. This scarcity of speeches, and so a shortage of number of sentences as well, amongst only two out of the six presidents results in an imbalanced corpus. The subsequent training of any ML model on such disproportionate data potentially jeopardizes the overall predictive performance and generalizability thereof.  Consequently, it is strategically chosen to exclude de Klerk and Motlanthe, hence instead focusing the prediction task exclusively on Mbeki, Zuma (with ten speeches each), Mandela and Ramaphosa (with seven speeches each). 


```{python}
#---------------------------------------------------------------------------------------------------------------------------
# exploratory data analysis: plot sentence counts 
#---------------------------------------------------------------------------------------------------------------------------
# count the occurrences of each president
president_num_sentences = sona_sentences['president'].value_counts().reset_index()
president_num_sentences.columns = ['president', 'num_sentences']

# setting a specific order for the presidents
ordered_presidents = ['Mandela', 'Mbeki', 'Zuma', 'Ramaphosa']
president_num_sentences['president'] = pd.Categorical(president_num_sentences['president'], categories=ordered_presidents, ordered=True)

# sorting the dataframe based on the defined order
president_num_sentences.sort_values('president', inplace=True)
president_num_sentences.reset_index(drop=True, inplace=True)

# creating the sentence-count plot
plt.bar(president_num_sentences['president'], president_num_sentences['num_sentences'], color=rdgy_palette[0]) 

# customizing the sentence-count plot
plt.xlabel("President", fontweight='bold', family='Andale Mono')
plt.ylabel("Number of Sentences", fontweight='bold', family='Andale Mono')
plt.xticks(rotation=55, horizontalalignment='right', fontweight='bold', family='Andale Mono')
plt.yticks(fontweight='bold', family='Andale Mono')
plt.title("", fontweight='bold', ha='center')
plt.grid(False)
plt.tight_layout()
plt.savefig(f'EDA/president_num_sentences.png', bbox_inches='tight')
plt.close() 
#---------------------------------------------------------------------------------------------------------------------------
```

![Number of sentences in SONA speeches for each president.](EDA/president_num_sentences.png){#fig-president_num_sentences}

@fig-president_num_sentences 

```{python}
#---------------------------------------------------------------------------------------------------------------------------
# exploratory data analysis:  plot average sentence lengths
#---------------------------------------------------------------------------------------------------------------------------
# # calculate the average length of sentences for each president
# sona_sentences['sentence_length'] = sona_sentences['sentence'].apply(len)
# avg_sentence_length = sona_sentences.groupby('president')['sentence_length'].mean().reset_index()
# avg_sentence_length['av_sen_length'] = avg_sentence_length['sentence_length'].apply(lambda x: int(x))
# avg_sentence_length.drop('sentence_length', axis=1, inplace=True)
# # sort the dataframe based on the defined order
# avg_sentence_length.sort_values('president', inplace=True)

# # plot the average sentence lengths 
# plt.bar(avg_sentence_length['president'], avg_sentence_length['av_sen_length'], color=rdgy_palette[9])

# # Customizing the plot
# plt.xlabel("President", fontweight='bold', family='Andale Mono')
# plt.ylabel("Mean Sentence Length (in words)", fontweight='bold', family='Andale Mono')
# plt.xticks(rotation=55, horizontalalignment='right', fontweight='bold', family='Andale Mono')
# plt.yticks(finallyontweight='bold', family='Andale Mono')
# plt.title("", fontweight='bold', ha='center')
# plt.grid(False)
# plt.tight_layout()
# plt.savefig(f'EDA/avg_sentence_length.png', bbox_inches='tight')
# plt.close() 
#---------------------------------------------------------------------------------------------------------------------------------------------------------
```

<!-- ![Average sentence length in SONA speeches for each president.](EDA/avg_sentence_length.png){#fig-avg_sentence_length} -->

@fig-avg_sentence_length illustrates a notable contrast in the textual characteristics of speeches by Mandela and Zuma. Although Mandela's speeches encompass the fewest sentences, they surprisingly contain the second-highest average word count per sentence. Conversely, Zuma's speeches, while having the highest sentence count, exhibit the shortest average sentence length.

```{python}
#---------------------------------------------------------------------------------------------------------------------------------------------------------
# exploratory data analysis: plot top frequent words across all speeches and stratified by president
#---------------------------------------------------------------------------------------------------------------------------------------------------------
# add a new column 'sentenceID' which is the row number
sona_sentences['sentenceID'] = range(1, len(sona_sentences) + 1)

# tokenize each sentence into words and create a new dataframe
sona_words = sona_sentences['cleaned_sentence'].apply(word_tokenize).explode().reset_index()
sona_words.columns = ['sentenceID', 'word']

# counting the occurrences of each word
word_counts = sona_words['word'].value_counts().reset_index()
word_counts.columns = ['word', 'n']

# find the top 15 most frequent words
top_words = word_counts.head(15)

# sort the words for better visualization
top_words = top_words.sort_values(by='n', ascending=True)

# plot the top 15 most frequent words across all speeches
plt.figure(figsize=(10, 6))
sns.barplot(x='n', y='word', data=top_words, color=rdgy_palette[0])  

# Customizing the plot
plt.xlabel("Number of times word appears", fontweight='bold')
plt.ylabel("", fontweight='bold', family='Andale Mono')
plt.xticks(fontweight='bold', family='Andale Mono')
plt.yticks(fontweight='bold', family='Andale Mono')
plt.title("", fontweight='bold', ha='center')
plt.grid(False)
sns.set_style("whitegrid")
plt.tight_layout()
plt.savefig(f'EDA/top_words.png', bbox_inches='tight')
plt.close() 

# # Assuming sona_words DataFrame is already defined and contains the 'word' column
# # Count the occurrences of each word
# word_counts_all = sona_words['word'].value_counts().head(15)

# # Sort the words for better visualization
# word_counts_all = word_counts_all.sort_values()

# # Define a RdGy-like color palette
# rdgy_palette = sns.color_palette("RdGy", n_colors=10)

# # Assuming 'president' is a column in the sona_words DataFrame
# # Filter the top 15 words for each president
# top_words_per_president = sona_words.groupby('president')['word'].value_counts().groupby(level=0).head(15).reset_index(name='count')

# # Create a facetted bar plot
# g = sns.catplot(x='count', y='word', col='president', col_wrap=4, data=top_words_per_president, kind='bar', palette=rdgy_palette)
# g.set_axis_labels("Number of times word appears", "")
# g.set_titles("{col_name}")
# plt.savefig(f'EDA/top_words_per_president.png', bbox_inches='tight')
# plt.close() 
# #---------------------------------------------------------------------------------------------------------------------------------------------------------
```


![Most frequent words stated across all speeches, irrespective of president.](EDA/top_words.png){#fig-top_words}
@fig-top_words


![Most frequent words stated across all speeches, stratified by president.](EDA/top_words_per_president.png){#fig-top_words_per_president}
@fig-top_words_per_president 


```{python}
#---------------------------------------------------------------------------------------------------------------------------
# data pre-processing: create three different data structures for analysis
#---------------------------------------------------------------------------------------------------------------------------
# create BoW ~ using top 150 words
bow_vectorizer = CountVectorizer(max_features=150)
bow_features = bow_vectorizer.fit_transform(sona_sentences['cleaned_sentence']).toarray()
bow_features.shape
# create tf-idf ~ using 150 words 
tfidf_vectorizer = TfidfVectorizer(max_features=150)
tfidf_features = tfidf_vectorizer.fit_transform(sona_sentences['cleaned_sentence']).toarray()

# create embeddings for other models
tokenized_speeches = [text.split() for text in sona_sentences['cleaned_sentence']]
model = Word2Vec(sentences=tokenized_speeches, vector_size=150, window=5, min_count=1, workers=4)
embeddings_features = np.array([np.mean([model.wv[word] for word in text.split() if word in model.wv] or [np.zeros(150)], axis=0) for text in sona_sentences['cleaned_sentence']])

def sentence_to_avg_vector(sentence, model):
    return np.mean([model.wv[word] for word in sentence.split() if word in model.wv] or [np.zeros(model.vector_size)], axis=0)

# cnvert sentences to sequences of vectors
vector_sequences = [[model.wv[word] for word in sentence.split() if word in model.wv] for sentence in sona_sentences['cleaned_sentence']]

# determine the length of the longest sequence
max_seq_length = max(len(sequence) for sequence in vector_sequences)

# pad sequences to have the same length
padded_sequences = pad_sequences(vector_sequences, maxlen=max_seq_length, padding='post', dtype='float32', value=0)

#---------------------------------------------------------------------------------------------------------------------------
```




```{python}
#---------------------------------------------------------------------------------------------------------------------------
# create splits for the data ~ 60-20-20 = training-validation-test
#---------------------------------------------------------------------------------------------------------------------------
seed = 5

# create data split for BoW approach
labels = sona_sentences['president']
X_train_bow, X_temp_bow, y_train, y_temp = train_test_split(bow_features, labels, test_size=0.4, random_state=seed, stratify=labels)
X_val_bow, X_test_bow, y_val, y_test = train_test_split(X_temp_bow, y_temp, test_size=0.5, random_state=seed, stratify=y_temp)

# create data split for tf-idf approach
X_train_tfidf, X_temp_tfidf = train_test_split(tfidf_features, test_size=0.4, random_state=seed, stratify=labels)
X_val_tfidf, X_test_tfidf = train_test_split(X_temp_tfidf, test_size=0.5, random_state=seed, stratify=y_temp)

# create data split for embedding approach
X_train_emb, X_temp_emb = train_test_split(embeddings_features, test_size=0.4, random_state=seed, stratify=labels)
X_val_emb, X_test_emb = train_test_split(X_temp_emb, test_size=0.5, random_state=seed, stratify=y_temp)

X_train_rnnemb, X_temp_rnnemb, y_train, y_temp = train_test_split(padded_sequences, labels, test_size=0.4, random_state=seed, stratify=labels)
X_val_rnnemb, X_test_rnnemb, y_val, y_test = train_test_split(X_temp_rnnemb, y_temp, test_size=0.5, random_state=seed, stratify=y_temp)

#---------------------------------------------------------------------------------------------------------------------------
```

```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Classification tree ~ define general function for all approaches
#---------------------------------------------------------------------------------------------------------------------------
def decision_tree_analysis(X_train, X_val, X_test, y_train, y_val, y_test, feature_set_name, results_dir='results'):
    # Define file paths
    model_path = os.path.join(results_dir, f'tree_clf_{feature_set_name}.pkl')
    plot_path = os.path.join(results_dir, f'cv_results_{feature_set_name}.png')

    # Check if results already exist
    if os.path.exists(model_path):
        tree_clf = joblib.load(model_path)
    else:
        # Hyperparameter grid search
        tree_params = {'max_depth': [3, 5, 7], 'min_samples_split': [2, 5, 10]}
        tree_clf = GridSearchCV(DecisionTreeClassifier(), tree_params, cv=5, n_jobs=-1)
        tree_clf.fit(X_train, y_train)

        # Save the trained model
        os.makedirs(results_dir, exist_ok=True)
        joblib.dump(tree_clf, model_path)

    # Predictions and evaluation on test set
    y_pred_tree_test = tree_clf.predict(X_test)
    print(f"Best parameters ({feature_set_name}):", tree_clf.best_params_)
    print(f"\nClassification Report on Test Set (Classification Tree - {feature_set_name}):")
    print(classification_report(y_test, y_pred_tree_test))

    # Predictions and evaluation on validation set
    y_pred_tree_val = tree_clf.predict(X_val)
    print(f"\nClassification Report on Validation Set (Classification Tree - {feature_set_name}):")
    print(classification_report(y_val, y_pred_tree_val))

    # Confusion matrix for test set
    conf_matrix_tree_test = confusion_matrix(y_test, y_pred_tree_test)
    sns.heatmap(conf_matrix_tree_test, annot=True, fmt='g', cmap=rdgy_palette)
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix on Test Set - {feature_set_name}')
    plt.show()

    # Extract and plot results
    results = tree_clf.cv_results_
    df = pd.DataFrame({
        'max_depth': results['param_max_depth'],
        'min_samples_split': results['param_min_samples_split'],
        'mean_test_score': results['mean_test_score']
    })
    pivoted = df.pivot(index='max_depth', columns='min_samples_split', values='mean_test_score')

    plt.figure(figsize=(8, 6))
    # Format the annotations to display up to four decimal places
    sns.heatmap(pivoted, annot=True, fmt=".4f", cmap=rdgy_palette)
    plt.title(f'Mean CV Score Heatmap - {feature_set_name}')
    plt.xlabel('Min Samples Split')
    plt.ylabel('Max Depth')
    plt.savefig(plot_path)
    plt.show()
#---------------------------------------------------------------------------------------------------------------------------
```

```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Classification tree  ~ for BoW approach
#---------------------------------------------------------------------------------------------------------------------------

decision_tree_analysis(X_train_bow, X_val_bow, X_test_bow, y_train, y_val, y_test, 'bow')

#---------------------------------------------------------------------------------------------------------------------------
```

```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Classification tree  ~ for tf-idf approach
#---------------------------------------------------------------------------------------------------------------------------

decision_tree_analysis(X_train_tfidf, X_val_tfidf, X_test_tfidf, y_train, y_val, y_test, 'tfidf')

#---------------------------------------------------------------------------------------------------------------------------
```


```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Classification tree  ~ for embedding approach
#--------------------------------------------------------------------------------------------------------------------------

decision_tree_analysis(X_train_emb, X_val_emb, X_test_emb, y_train, y_val, y_test, 'embedding')
#---------------------------------------------------------------------------------------------------------------------------
```


<!-- ::: {#fig-sent-presidents layout-ncol=2}

![$\texttt{AFINN}$: de Klerk speeches](sentiment_plots/sent_afinn_deKlerk.png){#fig-sent-deKlerk-afinn}

![$\texttt{bing}$: de Klerk speeches](sentiment_plots/sent_bing_deKlerk.png){#fig-sent-deKlerk-bing}

::: -->

```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Feed-forward neural network ~ define general function for all approaches
#---------------------------------------------------------------------------------------------------------------------------
def create_model(hidden_layer_sizes, activation, input_shape, num_classes):
    model = Sequential()
    for i, layer_size in enumerate(hidden_layer_sizes):
        if i == 0:
            model.add(Dense(layer_size, activation=activation, input_shape=input_shape))
        else:
            model.add(Dense(layer_size, activation=activation))
    model.add(Dense(num_classes, activation='softmax'))  # Output layer
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

def train_and_evaluate_model(X_train, X_val, X_test, y_train, y_val, y_test, hidden_layer_sizes, activation, feature_set_name, results_dir='model_results', epochs=100, batch_size=32):
    # Encoding target variables
    label_encoder = LabelEncoder()
    y_train_encoded = label_encoder.fit_transform(y_train)
    y_val_encoded = label_encoder.transform(y_val)
    y_test_encoded = label_encoder.transform(y_test)

    num_classes = len(np.unique(y_train_encoded))

    # Check if model and history already exist
    model_path = os.path.join(results_dir, f'model_{feature_set_name}_{hidden_layer_sizes}_{activation}.h5')
    history_path = os.path.join(results_dir, f'history_{feature_set_name}_{hidden_layer_sizes}_{activation}.pkl')

    if os.path.exists(model_path) and os.path.exists(history_path):
        model = tf.keras.models.load_model(model_path)
        history = joblib.load(history_path)
    else:
        # Create and train the model
        model = create_model(hidden_layer_sizes, activation, (X_train.shape[1],), num_classes)
        history = model.fit(X_train, y_train_encoded, validation_data=(X_val, y_val_encoded), epochs=epochs, batch_size=batch_size, verbose=0)

        # Save the model and history
        os.makedirs(results_dir, exist_ok=True)
        model.save(model_path)
        joblib.dump(history.history, history_path)

    
    # Check if history is a dict (loaded from file) or a History object (from training)
    if isinstance(history, dict):
        loss = history['loss']
        val_loss = history['val_loss']
    else:  # If it's a History object
        loss = history.history['loss']
        val_loss = history.history['val_loss']

    # Plot training & validation loss values
    plt.plot(loss)
    plt.plot(val_loss)
    plt.title(f'Model Loss - {feature_set_name} - {hidden_layer_sizes} - {activation}')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper left')
    plt.show()

    # Evaluate the model on the test set
    y_pred_encoded = np.argmax(model.predict(X_test), axis=1)
    y_pred = label_encoder.inverse_transform(y_pred_encoded)
    print(f"\nClassification Report ({feature_set_name} - {hidden_layer_sizes} - {activation}):")
    print(classification_report(y_test, y_pred))

    # Confusion Matrix
    conf_matrix = confusion_matrix(y_test, y_pred)
    sns.heatmap(conf_matrix, annot=True, fmt='g')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix - {feature_set_name} - {hidden_layer_sizes} - {activation}')
    plt.show()

    return model, history
#---------------------------------------------------------------------------------------------------------------------------
```


```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Feed-forward neural network ~ for BoW approach
#---------------------------------------------------------------------------------------------------------------------------

fnn_params = [(100,), (100, 50)]
activations = ['relu', 'tanh']

for hidden_layers in fnn_params:
    for activation in activations:
        print(f"\nTraining model with {hidden_layers} hidden layers and {activation} activation")
        train_and_evaluate_model(X_train_bow, X_val_bow, X_test_bow, y_train, y_val, y_test, hidden_layers, activation, 'bow')

#---------------------------------------------------------------------------------------------------------------------------
```


```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Feed-forward neural network ~ for tf-idf approach
#---------------------------------------------------------------------------------------------------------------------------

fnn_params = [(100,), (100, 50)]
activations = ['relu', 'tanh']

for hidden_layers in fnn_params:
    for activation in activations:
        print(f"\nTraining model with {hidden_layers} hidden layers and {activation} activation")
        train_and_evaluate_model(X_train_tfidf, X_val_tfidf, X_test_tfidf, y_train, y_val, y_test, hidden_layers, activation, 'tf-idf')


#---------------------------------------------------------------------------------------------------------------------------
```

```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Feed-forward neural network ~ for embedding approach
#---------------------------------------------------------------------------------------------------------------------------

fnn_params = [(100,), (100, 50)]
activations = ['relu', 'tanh']

for hidden_layers in fnn_params:
    for activation in activations:
        print(f"\nTraining model with {hidden_layers} hidden layers and {activation} activation")
        train_and_evaluate_model(X_train_emb, X_val_emb, X_test_emb, y_train, y_val, y_test, hidden_layers, activation, 'embedding')

#---------------------------------------------------------------------------------------------------------------------------
```


```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Convolutional neural network ~ define a general function for all approaches
#---------------------------------------------------------------------------------------------------------------------------
def create_cnn_model(input_shape, num_classes, filters, kernel_size, dropout_rate):
    model = Sequential()
    model.add(Input(shape=input_shape))  # Adjust to the shape of your features
    model.add(Reshape((input_shape[0], 1)))  # Add a reshape layer to make it compatible with Conv1D
    model.add(Conv1D(filters=filters, kernel_size=kernel_size, activation='relu'))
    model.add(GlobalMaxPooling1D())
    model.add(Dropout(rate=dropout_rate))
    model.add(Dense(num_classes, activation='softmax'))  # Output layer
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

def train_and_evaluate_cnn_model(X_train, X_val, X_test, y_train, y_val, y_test, filters, kernel_size, dropout_rate, feature_set_name, results_dir='model_results', epochs=100, batch_size=32):
    # Encoding target variables
    label_encoder = LabelEncoder()
    y_train_encoded = label_encoder.fit_transform(y_train)
    y_val_encoded = label_encoder.transform(y_val)
    y_test_encoded = label_encoder.transform(y_test)

    num_classes = len(np.unique(y_train_encoded))

    # Check if model and history already exist
    model_path = os.path.join(results_dir, f'cnn_model_{feature_set_name}_{filters}_{kernel_size}_{dropout_rate}.h5')
    history_path = os.path.join(results_dir, f'cnn_history_{feature_set_name}_{filters}_{kernel_size}_{dropout_rate}.pkl')

    if os.path.exists(model_path) and os.path.exists(history_path):
        model = tf.keras.models.load_model(model_path)
        history = joblib.load(history_path)
    else:
        # Create and train the model
        model = create_cnn_model((X_train.shape[1],), num_classes, filters, kernel_size, dropout_rate)
        history = model.fit(X_train, y_train_encoded, validation_data=(X_val, y_val_encoded), epochs=epochs, batch_size=batch_size, verbose=0)

        # Save the model and history
        os.makedirs(results_dir, exist_ok=True)
        model.save(model_path)
        joblib.dump(history.history, history_path)

    # Check if history is a dict (loaded from file) or a History object (from training)
    if isinstance(history, dict):
        loss = history['loss']
        val_loss = history['val_loss']
    else:  # If it's a History object
        loss = history.history['loss']
        val_loss = history.history['val_loss']

    # Plot training & validation loss values
    plt.plot(loss)
    plt.plot(val_loss)
    plt.title(f'CNN Model Loss - {feature_set_name} - Filters:{filters} Kernel:{kernel_size} Dropout:{dropout_rate}')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper left')
    plt.show()

    # Evaluate the model on the test set
    y_pred_encoded = np.argmax(model.predict(X_test), axis=1)
    y_pred = label_encoder.inverse_transform(y_pred_encoded)
    print(f"\nClassification Report (CNN - {feature_set_name} - Filters:{filters} Kernel:{kernel_size} Dropout:{dropout_rate}):")
    print(classification_report(y_test, y_pred))

    # Confusion Matrix
    conf_matrix = confusion_matrix(y_test, y_pred)
    sns.heatmap(conf_matrix, annot=True, fmt='g')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'CNN Confusion Matrix - {feature_set_name} - Filters:{filters} Kernel:{kernel_size} Dropout:{dropout_rate}')
    plt.show()

    return model, history


# Define a small set of hyperparameters for the search
filters_options = [64, 128]  # Number of filters
kernel_size_options = [3, 5]  # Size of the convolutional kernels
dropout_rate_options = [0.2, 0.5]  # Dropout rates

#---------------------------------------------------------------------------------------------------------------------------
```


```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Convolutional neural network ~ for BoW approach
#---------------------------------------------------------------------------------------------------------------------------
# Define other parameters
feature_set_name = "BoW" 
epochs = 10  
batch_size = 32 
results_dir = "cnn_bow_results"

# Prepare a list to store the results
search_results = []

# Iterate over all combinations of hyperparameters
for filters in filters_options:
    for kernel_size in kernel_size_options:
        for dropout_rate in dropout_rate_options:
            print(f"Training model with filters={filters}, kernel_size={kernel_size}, dropout_rate={dropout_rate}")

            # Train and evaluate the model
            model, history = train_and_evaluate_cnn_model(X_train_bow, X_val_bow, X_test_bow, y_train, y_val, y_test, filters, kernel_size, dropout_rate, feature_set_name, results_dir, epochs, batch_size)

            # Get the best validation accuracy from the history
            if isinstance(history, dict):
                best_val_accuracy = max(history['val_accuracy'])
            else:  # If it's a History object
                best_val_accuracy = max(history.history['val_accuracy'])

            # Store the results
            search_results.append({
                'filters': filters,
                'kernel_size': kernel_size,
                'dropout_rate': dropout_rate,
                'best_val_accuracy': best_val_accuracy
            })

# Sort the results by the best validation accuracy
search_results = sorted(search_results, key=lambda x: x['best_val_accuracy'], reverse=True)

# Print the results
for result in search_results:
    print(f"Filters: {result['filters']}, Kernel Size: {result['kernel_size']}, Dropout Rate: {result['dropout_rate']}, Best Val Accuracy: {result['best_val_accuracy']}")

#---------------------------------------------------------------------------------------------------------------------------
```


```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Convolutional neural network ~ for tf-idf approach
#---------------------------------------------------------------------------------------------------------------------------
# Define other parameters
feature_set_name = "tf-idf" 
epochs = 10  
batch_size = 32 
results_dir = "cnn_tfidf_results"

# Prepare a list to store the results
search_results = []

# Iterate over all combinations of hyperparameters
for filters in filters_options:
    for kernel_size in kernel_size_options:
        for dropout_rate in dropout_rate_options:
            print(f"Training model with filters={filters}, kernel_size={kernel_size}, dropout_rate={dropout_rate}")

            # Train and evaluate the model
            model, history = train_and_evaluate_cnn_model(X_train_tfidf, X_val_tfidf, X_test_tfidf, y_train, y_val, y_test, filters, kernel_size, dropout_rate, feature_set_name, results_dir, epochs, batch_size)

            # Get the best validation accuracy from the history
            if isinstance(history, dict):
                best_val_accuracy = max(history['val_accuracy'])
            else:  # If it's a History object
                best_val_accuracy = max(history.history['val_accuracy'])

            # Store the results
            search_results.append({
                'filters': filters,
                'kernel_size': kernel_size,
                'dropout_rate': dropout_rate,
                'best_val_accuracy': best_val_accuracy
            })

# Sort the results by the best validation accuracy
search_results = sorted(search_results, key=lambda x: x['best_val_accuracy'], reverse=True)

# Print the results
for result in search_results:
    print(f"Filters: {result['filters']}, Kernel Size: {result['kernel_size']}, Dropout Rate: {result['dropout_rate']}, Best Val Accuracy: {result['best_val_accuracy']}")
#---------------------------------------------------------------------------------------------------------------------------
```


```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Convolutional neural network ~ for embedding approach
#---------------------------------------------------------------------------------------------------------------------------
# Define other parameters
feature_set_name = "Embedding" 
epochs = 10  
batch_size = 32 
results_dir = "cnn_emb_results"

# Prepare a list to store the results
search_results = []

# Iterate over all combinations of hyperparameters
for filters in filters_options:
    for kernel_size in kernel_size_options:
        for dropout_rate in dropout_rate_options:
            print(f"Training model with filters={filters}, kernel_size={kernel_size}, dropout_rate={dropout_rate}")

            # Train and evaluate the model
            model, history = train_and_evaluate_cnn_model(X_train_emb, X_val_emb, X_test_emb, y_train, y_val, y_test, filters, kernel_size, dropout_rate, feature_set_name, results_dir, epochs, batch_size)

            # Get the best validation accuracy from the history
            if isinstance(history, dict):
                best_val_accuracy = max(history['val_accuracy'])
            else:  # If it's a History object
                best_val_accuracy = max(history.history['val_accuracy'])
            

            # Store the results
            search_results.append({
                'filters': filters,
                'kernel_size': kernel_size,
                'dropout_rate': dropout_rate,
                'best_val_accuracy': best_val_accuracy
            })

# Sort the results by the best validation accuracy
search_results = sorted(search_results, key=lambda x: x['best_val_accuracy'], reverse=True)

# Print the results
for result in search_results:
    print(f"Filters: {result['filters']}, Kernel Size: {result['kernel_size']}, Dropout Rate: {result['dropout_rate']}, Best Val Accuracy: {result['best_val_accuracy']}")
#---------------------------------------------------------------------------------------------------------------------------
```

```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Recurrent neural network ~ define a general function for all approaches
#---------------------------------------------------------------------------------------------------------------------------
def build_rnn_model(input_shape, num_classes, lstm_units, spatial_dropout, dropout, recurrent_dropout):
    model = Sequential()
    # Reshape layer to convert 2D input to 3D
    model.add(Reshape((input_shape[0], 1), input_shape=input_shape)) 
    model.add(SpatialDropout1D(spatial_dropout))
    model.add(LSTM(units=lstm_units, dropout=dropout, recurrent_dropout=recurrent_dropout))
    model.add(Dense(units=num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

def build_rnn_model2(num_classes, lstm_units, spatial_dropout, dropout, recurrent_dropout, input_shape):
    model = Sequential()
    model.add(SpatialDropout1D(spatial_dropout, input_shape=input_shape)) 
    model.add(LSTM(units=lstm_units, dropout=dropout, recurrent_dropout=recurrent_dropout))
    model.add(Dense(units=num_classes, activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

def train_and_evaluate_rnn_model(X_train, X_val, X_test, y_train, y_val, y_test, lstm_units, spatial_dropout, dropout, recurrent_dropout, feature_set_name, results_dir='model_results', epochs=100, batch_size=32):
    # Encoding target variables
    label_encoder = LabelEncoder()
    y_train_encoded = to_categorical(label_encoder.fit_transform(y_train))
    y_val_encoded = to_categorical(label_encoder.transform(y_val))
    y_test_encoded = to_categorical(label_encoder.transform(y_test))

    num_classes = y_train_encoded.shape[1]

    # Check if model and history already exist
    model_path = os.path.join(results_dir, f'rnn_model_{feature_set_name}_{lstm_units}_{spatial_dropout}_{dropout}_{recurrent_dropout}.h5')
    history_path = os.path.join(results_dir, f'rnn_history_{feature_set_name}_{lstm_units}_{spatial_dropout}_{dropout}_{recurrent_dropout}.pkl')

    if os.path.exists(model_path) and os.path.exists(history_path):
        model = tf.keras.models.load_model(model_path)
        history = joblib.load(history_path)
    else:
        # Create and train the model
        if feature_set_name == 'Embedding':
            input_shape = (X_train.shape[1], X_train.shape[2])  # (118, 150)
            model = build_rnn_model2(num_classes, lstm_units, spatial_dropout, dropout, recurrent_dropout, input_shape)
        else:
            model = build_rnn_model((X_train.shape[1],), num_classes, lstm_units, spatial_dropout, dropout, recurrent_dropout)
        history = model.fit(X_train, y_train_encoded, validation_data=(X_val, y_val_encoded), epochs=epochs, batch_size=batch_size, verbose=0)

        # Save the model and history
        os.makedirs(results_dir, exist_ok=True)
        model.save(model_path)
        joblib.dump(history.history, history_path)

    # Plotting training & validation accuracy values
    if isinstance(history, dict):
        accuracy = history['accuracy']
        val_accuracy = history['val_accuracy']
    else:  # If it's a History object
        accuracy = history.history['accuracy']
        val_accuracy = history.history['val_accuracy']

    plt.plot(accuracy)
    plt.plot(val_accuracy)
    plt.title(f'RNN Model Accuracy - {feature_set_name} - LSTM:{lstm_units} SpatialDropout:{spatial_dropout} Dropout:{dropout} RecurrentDropout:{recurrent_dropout}')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper left')
    plt.show()

    # Evaluate the model on the test set
    y_pred_encoded = np.argmax(model.predict(X_test), axis=1)
    y_pred = label_encoder.inverse_transform(y_pred_encoded)
    print(f"\nClassification Report (RNN - {feature_set_name} - LSTM:{lstm_units} SpatialDropout:{spatial_dropout} Dropout:{dropout} RecurrentDropout:{recurrent_dropout}):")
    print(classification_report(y_test, y_pred))

    # Confusion Matrix
    conf_matrix = confusion_matrix(y_test, y_pred)
    sns.heatmap(conf_matrix, annot=True, fmt='g')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'RNN Confusion Matrix - {feature_set_name} - LSTM:{lstm_units} SpatialDropout:{spatial_dropout} Dropout:{dropout} RecurrentDropout:{recurrent_dropout}')
    plt.show()

    return model, history

#---------------------------------------------------------------------------------------------------------------------------
```


```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Recurrent neural network ~ for BoW approach
#---------------------------------------------------------------------------------------------------------------------------

# Define other parameters
feature_set_name = "BoW" 
epochs = 10  
batch_size = 32 
results_dir = "rnn_bow_results"

# Hyperparameter options for RNN
lstm_units_options = [50, 75]
spatial_dropout_options = [0.0, 0.2]
dropout_options = [0.2]
recurrent_dropout_options = [0.2]

# Prepare a list to store the results
search_results = []

# Iterate over all combinations of hyperparameters
for lstm_units in lstm_units_options:
    for spatial_dropout in spatial_dropout_options:
        for dropout in dropout_options:
            for recurrent_dropout in recurrent_dropout_options:

                # Train and evaluate the model
                model, history = train_and_evaluate_rnn_model(X_train_bow, X_val_bow, X_test_bow, y_train, y_val, y_test, lstm_units, spatial_dropout, dropout, recurrent_dropout, feature_set_name, results_dir, epochs, batch_size)

                # Get the best validation accuracy from the history
                if isinstance(history, dict):
                    best_val_accuracy = max(history['val_accuracy'])
                else:  # If it's a History object
                    best_val_accuracy = max(history.history['val_accuracy'])

                # Store the results
                search_results.append({
                    'lstm_units': lstm_units,
                    'spatial_dropout': spatial_dropout,
                    'dropout': dropout,
                    'recurrent_dropout': recurrent_dropout,
                    'best_val_accuracy': best_val_accuracy
                })

# Sort the results by the best validation accuracy
search_results = sorted(search_results, key=lambda x: x['best_val_accuracy'], reverse=True)

# Print the results
for result in search_results:
    print(f"LSTM Units: {result['lstm_units']}, Spatial Dropout: {result['spatial_dropout']}, Dropout: {result['dropout']}, Recurrent Dropout: {result['recurrent_dropout']}, Best Val Accuracy: {result['best_val_accuracy']}")

#---------------------------------------------------------------------------------------------------------------------------
```



```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Recurrent neural network ~ for tf-idf approach
#---------------------------------------------------------------------------------------------------------------------------
# Define other parameters
feature_set_name = "tf-idf" 
epochs = 10  
batch_size = 32 
results_dir = "rnn_tfidf_results"

# Hyperparameter options for RNN
lstm_units_options = [50, 75]
spatial_dropout_options = [0.0, 0.2]
dropout_options = [0.2]
recurrent_dropout_options = [0.2]

# Prepare a list to store the results
search_results = []

# Iterate over all combinations of hyperparameters

for lstm_units in lstm_units_options:
    for spatial_dropout in spatial_dropout_options:
        for dropout in dropout_options:
            for recurrent_dropout in recurrent_dropout_options:

                # Train and evaluate the model
                model, history = train_and_evaluate_rnn_model(X_train_tfidf, X_val_tfidf, X_test_tfidf, y_train, y_val, y_test, lstm_units, spatial_dropout, dropout, recurrent_dropout, feature_set_name, results_dir, epochs, batch_size)

                # Get the best validation accuracy from the history
                if isinstance(history, dict):
                    best_val_accuracy = max(history['val_accuracy'])
                else:  # If it's a History object
                    best_val_accuracy = max(history.history['val_accuracy'])

                # Store the results
                search_results.append({
                    'lstm_units': lstm_units,
                    'spatial_dropout': spatial_dropout,
                    'dropout': dropout,
                    'recurrent_dropout': recurrent_dropout,
                    'best_val_accuracy': best_val_accuracy
                })

# Sort the results by the best validation accuracy
search_results = sorted(search_results, key=lambda x: x['best_val_accuracy'], reverse=True)

# Print the results
for result in search_results:
    print(f"LSTM Units: {result['lstm_units']}, Spatial Dropout: {result['spatial_dropout']}, Dropout: {result['dropout']}, Recurrent Dropout: {result['recurrent_dropout']}, Best Val Accuracy: {result['best_val_accuracy']}")
#---------------------------------------------------------------------------------------------------------------------------
```



```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Recurrent neural network ~ for embedding approach
#---------------------------------------------------------------------------------------------------------------------------
# # Define other parameters
# feature_set_name = "Embedding" 
# epochs = 10  
# batch_size = 32 
# results_dir = "rnn_emb_results"

# # Hyperparameter options for RNN
# lstm_units_options = [50, 75]
# spatial_dropout_options = [0.0, 0.2]
# dropout_options = [0.2]
# recurrent_dropout_options = [0.2]

# # Prepare a list to store the results
# search_results = []

# # Iterate over all combinations of hyperparameters
# for lstm_units in lstm_units_options:
#     for spatial_dropout in spatial_dropout_options:
#         for dropout in dropout_options:
#             for recurrent_dropout in recurrent_dropout_options:

#                 # Train and evaluate the model
#                 model, history = train_and_evaluate_rnn_model(X_train_rnnemb, X_val_rnnemb, X_test_rnnemb, y_train, y_val, y_test, lstm_units, spatial_dropout, dropout, recurrent_dropout, feature_set_name, results_dir, epochs, batch_size)

#                 # Get the best validation accuracy from the history
#                 if isinstance(history, dict):
#                     best_val_accuracy = max(history['val_accuracy'])
#                 else:  # If it's a History object
#                     best_val_accuracy = max(history.history['val_accuracy'])

#                 # Store the results
#                 search_results.append({
#                     'lstm_units': lstm_units,
#                     'spatial_dropout': spatial_dropout,
#                     'dropout': dropout,
#                     'recurrent_dropout': recurrent_dropout,
#                     'best_val_accuracy': best_val_accuracy
#                 })

# # Sort the results by the best validation accuracy
# search_results = sorted(search_results, key=lambda x: x['best_val_accuracy'], reverse=True)

# # Print the results
# for result in search_results:
#     print(f"LSTM Units: {result['lstm_units']}, Spatial Dropout: {result['spatial_dropout']}, Dropout: {result['dropout']}, Recurrent Dropout: {result['recurrent_dropout']}, Best Val Accuracy: {result['best_val_accuracy']}")
#---------------------------------------------------------------------------------------------------------------------------
```


<h5> Overall model performance </h5>


<h4> Discussion and Conclusion </h4> 
<hr> 


<h4> References </h4> 
<hr> 

</div> 
