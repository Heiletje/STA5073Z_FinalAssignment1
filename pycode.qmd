---
title: "Prediction of Presidents"
title-size: small
format: html
execute:
  echo: false
  cache: true
  freeze: auto
---


<div style="text-align: justify"> 

<h4> Abstract </h4> 
<hr> 
Converging machine learning (ML) and natural language processing (NLP) techniques, a predictive text-analysis was performed on the State of the Nation Addresses (SONAs) delivered by South African presidents spanning the period from 1994 to 2023. Different types of neural networks (NNs) were employed for the purpose of predicting the president from their spoken sentences. 
Serving as a foundational-comparative model, a naive classification tree (CT) was first fitted followed by varying-in-complexity models from a simple feed-forward NN (FNN) to more sophisticated NNs, such as recurrent (RNN) and convolutional (CNN). For each predictive model, the involved text was transformed into features via three approaches, namely bag-of-words (BoW), term-frequency, inverse-document-frequency (tf-idf), and embeddings. A comparison, in terms of correct-classification capability, between the numerous NNs across the differently defined features was conducted with the objective of obtaining the optimal predictive model for the political speeches. 


<h4> Introduction </h4> 
<hr> 
The domains of machine learning (ML) and natural language processing (NLP) intersect at text-based analysis involving the underlying aim of prediction. In this instance, the text of interest to analyze are the annual State of the Nation Address (SONA) speeches (from 1994 to 2023), whilst the predictive objective is pointed at determining which one of the six different South African presidents (F.W. de Klerk, N.R. Mandela, T.M. Mbeki, K.P. Motlanthe, J.G. Zuma, and M.C. Ramaphosa) delivered some sentence contained therein. 

In order to achieve this predictive analysis, diverse ML models with differing degrees of complexity are applied. First, in order to form a comparative basis for performance, a naïve classification tree is fitted. This is then followed with the implementation of three types of neural networks (NNs),each with ascending degrees of sophistication, specifically a simple one (feed-forward) and then more complex ones (recurrent and convolutional).



<h4> Literature Review </h4> 
<hr> 

<b><u> SONA  </b></u>




<h4> Data </h4>
<hr>

<b><u> Tokenization </b></u>

<b><u> Pre-processing </b></u>


<h4> Methods </h4>
<hr>

<b><u> Classification Tree (CT) </b></u>
![Schematic representation of an exemplary CT.](CT.png){#fig-CT}

A classification tree (CT) [@Breiman1984] is a straightforward ML technique predominately applied in categorical prediction contexts. This simple model operates on the principle of segmenting some data into subsets, where these partitions are based on distinct values of individual features. It is naïve in its approach given it considers each feature in isolation in order to make classifying decisions, hence often overlooking complex feature interactions. 

When applied to text-based data, a CT makes classification decisions based on either the frequency (a BoW representation) or weighted frequency of words (a tf-idf representation) as the features. As illustrated in @fig-CT, each CT decision node (shown as a grey, rectangular block) represents a rule where some threshold on a particular word's frequency (or weighted version thereof) is defined. While less conventional, word embeddings can also be utilized instead. Though, the CT is often found to be mediocre in its ability to fully exploit the rich semantic and syntactic relationships captured in embeddings. Given that embeddings provide a dense representation of words in a high-dimensional space, the simplistic decision boundaries of a CT are sub-par in capturing the associated contextual nuances [@Charbuty2021]. 

CT hyperparameters comprise of the minimum number of samples required to split a node  as well as the maximum tree depth. Using a grid-search approach coupled with a *k*-fold cross-validation (CV) process, various combinations of these two hyperparameters over smaller sets of data (folds) are systematically assessed to find a configuration where a chosen performance metric is optimized. Here, such a metric is the average of evaluation scores (like accuracy) obtained across all folds for the hyperparameter set yielding the highest value thereof.

CTs are superior in terms of its simplistic interpretability and visualization, though this advantage is tainted with its proneness to overfitting (with deep trees) and inability to capture feature complexity.  

<b><u> Feed-forward Neural Network (NN) </b></u>
![Schematic representation of any FNN](FNN.png){#fig-FNN}

A feed-forward Neural Network (FNN) is a type of artificial NN wherein connections between the nodes (neurons) do not form a cycle. As depicted in @fig-FNN, this architecture is characterized by its stratified structure,  comprising of an input layer (grey circles), one or more hidden layers (white circles), and an output layer (black circles). In each layer, the nodes apply weights and biases to inputs before passing through an activation function.

In text analysis, when using BoW or tf-idf representations, each word is represented as a feature (i.e., a unique input neuron). FNNs transform text-based data into numerical vectors where each dimension corresponds to a word frequency (or weighted version thereof). These vectors are efficiently processed, thereby enabling the network to learn patterns that are predictive of an outcome of interest (like predicting presidents from sentences). With  the employment of embeddings, unlike CTs, FNNs are able to leverage such dense word representations to capture subtler semantic patterns, potentially leading to more enhanced predictions. 

FNN hyperparameters involve the number of hidden layers, the number of neurons in each hidden layer, the type of activation function (e.g., ReLU or tanh), and the learning rate. To identify the optimal network architecture, a grid search over these tuneable parameters is executed. 

In contrast to CTs, FNNs possess a higher aptitude to deal with feature intricacies, whether there is high dimensionality or the presence of non-linear relationships. Despite this, FNNs are also still susceptible to overfitting (with many layers/nodes). Additionally, despite its ability to capture semantic information, this is hindered by its inherent incapacity to simultaneously encapsulate the sequential/temporal relationship prevalent in text. 


<b><u> Convolutional Neural Network (CNN) </b></u>
![Schematic representation of any CNN](CNN.png){#fig-CNN}

Convolutional Neural Networks (CNNs), primarily applied within the field of image-processing, have also proven proficiency in processing natural language processing (NLP) tasks. CNNs are designed to adaptively learn spatial hierarchies of features from input data. 
In the context of text, this means the model is able to learn any recognizable patterns or sequences of words which carry significant semantic meaning.

In comparison to CTs and FNNS, CNNs are the most specialized to incorporate embeddings, exploiting the associated representation to capture contextual information. The convolutional layers (as shown in @fig-CNN) are able to detect local patterns (like phrases or commonly co-occurring words) within these embeddings. With this model, the BoW and tf-idf approach is not ordinarily applied, given that both these representations lack the inherent sequential nature of text. Thus, the ability of CNNs to capture local dependencies is limited. 

CNN hyperparameters consist of the number convolutional layers, the size and number of filters (kernels), the size of the pooling layers, and the dropout rate. To configure the optimal set of these tunable parameters, a grid search approach is applied. 

CNNs are able to capture local dependencies in text-based data, allowing for more apt short-length pattern recognition. However, when dealing with long-range dependencies (like with sentences), the performance of CNNs is dampened. Lastly, given the implicit shared-weight architecture of the model, the tendency to overfit is not as prevalent compared to CTs and FNNS. 


<b><u> Recurrent Neural Network (RNN) </b></u>
![Schematic representation of any RNN](RNN.png){#fig-RNN}

<b><u> Model performance metrics </b></u>



```{python}
#---------------------------------------------------------------------------------------------------------------------------
# preliminaries: load relevant libraries; import data; define colour palette
#---------------------------------------------------------------------------------------------------------------------------
# load libraries
import brewer2mpl
import pandas as pd
from nltk.tokenize import sent_tokenize
from sklearn.model_selection import train_test_split
import re
import numpy as np
import nltk
import tensorflow as tf
import random
import os
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from gensim.models import Word2Vec
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPClassifier
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout
from tensorflow.keras.utils import to_categorical
from kerastuner.tuners import RandomSearch
from tensorflow.keras.layers import LSTM, SpatialDropout1D
import matplotlib.ticker as ticker
from nltk.tokenize import word_tokenize
from keras.layers import Input, Conv1D, GlobalMaxPooling1D, Dropout, Dense, Reshape
from keras.models import Sequential
from keras_tuner import RandomSearch

# set global parameters
plt.rcParams['font.family'] = 'Andale Mono'
plt.rcParams['xtick.labelsize'] = 14
plt.rcParams['ytick.labelsize'] = 14
plt.rcParams['axes.labelsize'] = 16
plt.rcParams['legend.fontsize'] = 14

# set seeds for reproducibility purposes
np.random.seed(5)
tf.random.set_seed(5)
random.seed(5)
os.environ['PYTHONHASHSEED'] = str(5)

# import data 
data = pd.read_csv("sona.csv")

# generate the RdGy colour palette
num_colors = 10
rdgy_palette = brewer2mpl.get_map('RdGy', 'Diverging', num_colors, reverse=True).mpl_colors

#---------------------------------------------------------------------------------------------------------------------------
```


```{python}
#---------------------------------------------------------------------------------------------------------------------------
# data pre-processing: prepare data  ~ subsettting by presidents, cleaning, and segmenting speeches into sentences
#---------------------------------------------------------------------------------------------------------------------------

import pandas as pd
import nltk
from nltk.tokenize import sent_tokenize
from nltk.corpus import stopwords
import re

# select subset of four out of six presidents
subset = data[data['president'].isin(['Mandela', 'Mbeki', 'Zuma', 'Ramaphosa'])]

# initialize a list to store the sentences
sentences_data = []

# iterate through each row in the subset
for index, row in subset.iterrows():
    # split the speech into sentences
    speech_sentences = sent_tokenize(row['speech'])
    
    # for each sentence, create a new row with the same information
    for sentence in speech_sentences:
        sentences_data.append({
            'sentence': sentence,
            'year': row['year'],
            'president': row['president'],
            'date': row['date']
        })

# create a new dataframe with sentences
sona_sentences = pd.DataFrame(sentences_data)

# filtering function to remove stop words and only words with a length of three characters or more
english_words = set(nltk.corpus.words.words())
stop_words = set(stopwords.words('english'))

def filter_text(text):
    return ' '.join([word for word in text.split() if word not in stop_words and len(word) > 3 and word in english_words])

# apply the filter function to the cleaned sentences
sona_sentences['cleaned_sentence'] = sona_sentences['sentence'].apply(filter_text)

# clean sentences
sona_sentences['cleaned_sentence'] = sona_sentences['cleaned_sentence'].apply(lambda text: re.sub(r'[^A-Za-z\s]', '', text).lower())

#---------------------------------------------------------------------------------------------------------------------------
```


<h4> Exploratory Data Analysis </h4> 
<hr> 

Figure 1 


```{python}
#----------------------------------------------------------------------------------------------------------------------------
# exploratory data analysis: plot speech counts
#----------------------------------------------------------------------------------------------------------------------------
# create dataframe with counts of speeches for each president
president_num_speeches = data['president'].value_counts().reset_index()
president_num_speeches.columns = ['president', 'num_speeches']

# setting a specific order for the presidents
ordered_presidents = ['Mandela', 'Mbeki', 'Zuma', 'Ramaphosa']
president_num_speeches['president'] = pd.Categorical(president_num_speeches['president'], categories=ordered_presidents, ordered=True)

# sort the DataFrame based on the defined order
president_num_speeches.sort_values('president', inplace=True)

# plot the sentence counts
plt.bar(president_num_speeches['president'], president_num_speeches['num_speeches'], color=rdgy_palette[2])

# Customizing the plot
plt.xlabel("President", fontweight='bold', family='Andale Mono')
plt.ylabel("Number of Speeches", fontweight='bold', family='Andale Mono')
plt.xticks(rotation=55, horizontalalignment='right', fontweight='bold', family='Andale Mono')
plt.yticks(fontweight='bold', family='Andale Mono')
plt.title("", fontweight='bold', ha='center')

# Additional theme customizations
plt.grid(False)
plt.tight_layout()
plt.savefig(f'EDA/president_num_speeches.png', bbox_inches='tight')
plt.close() 
#---------------------------------------------------------------------------------------------------------------------------
```


![Number of SONA speeches for each president.](EDA/president_num_speeches.png){#fig-president_num_speeches}

@fig-president_num_speeches delineates the distribution of speeches across the different presidents, revealing a notable underrepresentation of de Klerk and Motlanthe. This scarcity of speeches, and so a shortage of number of sentences as well, amongst only two out of the six presidents results in an imbalanced corpus. The subsequent training of any ML model on such disproportionate data potentially jeopardizes the overall predictive performance and generalizability thereof.  Consequently, it is strategically chosen to exclude de Klerk and Motlanthe, hence instead focusing the prediction task exclusively on Mbeki, Zuma (with ten speeches each), Mandela and Ramaphosa (with seven speeches each). 


```{python}
#---------------------------------------------------------------------------------------------------------------------------
# exploratory data analysis: plot sentence counts 
#---------------------------------------------------------------------------------------------------------------------------
# count the occurrences of each president
president_num_sentences = sona_sentences['president'].value_counts().reset_index()
president_num_sentences.columns = ['president', 'num_sentences']

# setting a specific order for the presidents
ordered_presidents = ['Mandela', 'Mbeki', 'Zuma', 'Ramaphosa']
president_num_sentences['president'] = pd.Categorical(president_num_sentences['president'], categories=ordered_presidents, ordered=True)

# sorting the dataframe based on the defined order
president_num_sentences.sort_values('president', inplace=True)
president_num_sentences.reset_index(drop=True, inplace=True)

# creating the sentence-count plot
plt.bar(president_num_sentences['president'], president_num_sentences['num_sentences'], color=rdgy_palette[0]) 

# customizing the sentence-count plot
plt.xlabel("President", fontweight='bold', family='Andale Mono')
plt.ylabel("Number of Sentences", fontweight='bold', family='Andale Mono')
plt.xticks(rotation=55, horizontalalignment='right', fontweight='bold', family='Andale Mono')
plt.yticks(fontweight='bold', family='Andale Mono')
plt.title("", fontweight='bold', ha='center')
plt.grid(False)
plt.tight_layout()
plt.savefig(f'EDA/president_num_sentences.png', bbox_inches='tight')
plt.close() 
#---------------------------------------------------------------------------------------------------------------------------
```

![Number of sentences in SONA speeches for each president.](EDA/president_num_sentences.png){#fig-president_num_sentences}

@fig-president_num_sentences 

```{python}
#---------------------------------------------------------------------------------------------------------------------------
# exploratory data analysis:  plot average sentence lengths
#---------------------------------------------------------------------------------------------------------------------------
# calculate the average length of sentences for each president
sona_sentences['sentence_length'] = sona_sentences['sentence'].apply(len)
avg_sentence_length = sona_sentences.groupby('president')['sentence_length'].mean().reset_index()
avg_sentence_length['av_sen_length'] = avg_sentence_length['sentence_length'].apply(lambda x: int(x))
avg_sentence_length.drop('sentence_length', axis=1, inplace=True)
# sort the dataframe based on the defined order
avg_sentence_length.sort_values('president', inplace=True)

# plot the average sentence lengths 
plt.bar(avg_sentence_length['president'], avg_sentence_length['av_sen_length'], color=rdgy_palette[9])

# Customizing the plot
plt.xlabel("President", fontweight='bold', family='Andale Mono')
plt.ylabel("Mean Sentence Length (in words)", fontweight='bold', family='Andale Mono')
plt.xticks(rotation=55, horizontalalignment='right', fontweight='bold', family='Andale Mono')
plt.yticks(finallyontweight='bold', family='Andale Mono')
plt.title("", fontweight='bold', ha='center')
plt.grid(False)
plt.tight_layout()
plt.savefig(f'EDA/avg_sentence_length.png', bbox_inches='tight')
plt.close() 
#---------------------------------------------------------------------------------------------------------------------------------------------------------
```

![Average sentence length in SONA speeches for each president.](EDA/avg_sentence_length.png){#fig-avg_sentence_length}

@fig-avg_sentence_length illustrates a notable contrast in the textual characteristics of speeches by Mandela and Zuma. Although Mandela's speeches encompass the fewest sentences, they surprisingly contain the second-highest average word count per sentence. Conversely, Zuma's speeches, while having the highest sentence count, exhibit the shortest average sentence length.

```{python}
#---------------------------------------------------------------------------------------------------------------------------------------------------------
# exploratory data analysis: plot top frequent words across all speeches and stratified by president
#---------------------------------------------------------------------------------------------------------------------------------------------------------
# add a new column 'sentenceID' which is the row number
sona_sentences['sentenceID'] = range(1, len(sona_sentences) + 1)

# tokenize each sentence into words and create a new dataframe
sona_words = sona_sentences['cleaned_sentence'].apply(word_tokenize).explode().reset_index()
sona_words.columns = ['sentenceID', 'word']

# counting the occurrences of each word
word_counts = sona_words['word'].value_counts().reset_index()
word_counts.columns = ['word', 'n']

# find the top 15 most frequent words
top_words = word_counts.head(15)

# sort the words for better visualization
top_words = top_words.sort_values(by='n', ascending=True)

# plot the top 15 most frequent words across all speeches
plt.figure(figsize=(10, 6))
sns.barplot(x='n', y='word', data=top_words, color=rdgy_palette[0])  

# Customizing the plot
plt.xlabel("Number of times word appears", fontweight='bold')
plt.ylabel("", fontweight='bold', family='Andale Mono')
plt.xticks(fontweight='bold', family='Andale Mono')
plt.yticks(fontweight='bold', family='Andale Mono')
plt.title("", fontweight='bold', ha='center')
plt.grid(False)
sns.set_style("whitegrid")
plt.tight_layout()
plt.savefig(f'EDA/top_words.png', bbox_inches='tight')
plt.close() 

# Assuming sona_words DataFrame is already defined and contains the 'word' column
# Count the occurrences of each word
word_counts_all = sona_words['word'].value_counts().head(15)

# Sort the words for better visualization
word_counts_all = word_counts_all.sort_values()

# Define a RdGy-like color palette
rdgy_palette = sns.color_palette("RdGy", n_colors=10)

# Assuming 'president' is a column in the sona_words DataFrame
# Filter the top 15 words for each president
top_words_per_president = sona_words.groupby('president')['word'].value_counts().groupby(level=0).head(15).reset_index(name='count')

# Create a facetted bar plot
g = sns.catplot(x='count', y='word', col='president', col_wrap=4, data=top_words_per_president, kind='bar', palette=rdgy_palette)
g.set_axis_labels("Number of times word appears", "")
g.set_titles("{col_name}")
plt.savefig(f'EDA/top_words_per_president.png', bbox_inches='tight')
plt.close() 
#---------------------------------------------------------------------------------------------------------------------------------------------------------
```


![Most frequent words stated across all speeches, irrespective of president.](EDA/top_words.png){#fig-top_words}
@fig-top_words


![Most frequent words stated across all speeches, stratified by president.](EDA/top_words_per_president.png){#fig-top_words_per_president}
@fig-top_words_per_president 



```{python}
#---------------------------------------------------------------------------------------------------------------------------
# data pre-processing: create three different data structures for analysis
#---------------------------------------------------------------------------------------------------------------------------
# create BoW ~ using top 150 words
bow_vectorizer = CountVectorizer(max_features=150)
bow_features = bow_vectorizer.fit_transform(sona_sentences['cleaned_sentence']).toarray()
bow_features.shape
# create tf-idf ~ using 150 words 
tfidf_vectorizer = TfidfVectorizer(max_features=150)
tfidf_features = tfidf_vectorizer.fit_transform(sona_sentences['cleaned_sentence']).toarray()

# create embeddings
tokenized_speeches = [text.split() for text in sona_sentences['cleaned_sentence']]
model = Word2Vec(sentences=tokenized_speeches, vector_size=150, window=5, min_count=1, workers=4)
embeddings_features = np.array([np.mean([model.wv[word] for word in text.split() if word in model.wv] or [np.zeros(150)], axis=0) for text in sona_sentences['cleaned_sentence']])
#---------------------------------------------------------------------------------------------------------------------------
```


```{python}
#---------------------------------------------------------------------------------------------------------------------------
# create splits for the data ~ 60-20-20 = training-validation-test
#---------------------------------------------------------------------------------------------------------------------------
seed = 5

# create data split for BoW approach
labels = sona_sentences['president']
X_train_bow, X_temp_bow, y_train, y_temp = train_test_split(bow_features, labels, test_size=0.4, random_state=seed, stratify=labels)
X_val_bow, X_test_bow, y_val, y_test = train_test_split(X_temp_bow, y_temp, test_size=0.5, random_state=seed, stratify=y_temp)

# create data split for tf-idf approach
X_train_tfidf, X_temp_tfidf = train_test_split(tfidf_features, test_size=0.4, random_state=seed, stratify=labels)
X_val_tfidf, X_test_tfidf = train_test_split(X_temp_tfidf, test_size=0.5, random_state=seed, stratify=y_temp)

# create data split for embedding approach
X_train_emb, X_temp_emb = train_test_split(embeddings_features, test_size=0.4, random_state=seed, stratify=labels)
X_val_emb, X_test_emb = train_test_split(X_temp_emb, test_size=0.5, random_state=seed, stratify=y_temp)


#---------------------------------------------------------------------------------------------------------------------------
```

```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Classification tree  ~ for BoW approach
#---------------------------------------------------------------------------------------------------------------------------
tree_params = {'max_depth': [3, 5, 10], 'min_samples_split': [2, 5, 10]}
# hyperparameter gridsearch 
tree_clf_bow = GridSearchCV(DecisionTreeClassifier(), tree_params, cv=5) 
tree_clf_bow.fit(X_train_bow, y_train)

# predictions and evaluation
y_pred_tree_bow = tree_clf_bow.predict(X_test_bow)
print("Best parameters:", tree_clf_bow.best_params_)
print("\nClassification Report (Classification Tree - BoW):")
print(classification_report(y_test, y_pred_tree_bow))

# confusion matrix
conf_matrix_tree_bow = confusion_matrix(y_test, y_pred_tree_bow)
sns.heatmap(conf_matrix_tree_bow, annot=True, fmt='g',cmap=rdgy_palette)
plt.xlabel('Predicted', fontweight='bold', family='Andale Mono')
plt.ylabel('Actual', fontweight='bold', family='Andale Mono')
plt.show()
#---------------------------------------------------------------------------------------------------------------------------
```

```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Classification tree  ~ for tf-idf approach
#---------------------------------------------------------------------------------------------------------------------------
tree_params = {'max_depth': [3, 5, 10], 'min_samples_split': [2, 5, 10]}
# hyperparameter gridsearch 
tree_clf_tfidf = GridSearchCV(DecisionTreeClassifier(), tree_params, cv=5) 
tree_clf_tfidf.fit(X_train_tfidf, y_train)

# predictions and evaluation
y_pred_tree_tfidf = tree_clf_tfidf.predict(X_test_tfidf)
print("Best parameters:", tree_clf_tfidf.best_params_)
print("\nClassification Report (Classification Tree - tf-idf):")
print(classification_report(y_test, y_pred_tree_tfidf))

# confusion matrix
conf_matrix_tree_tfidf = confusion_matrix(y_test, y_pred_tree_tfidf)
sns.heatmap(conf_matrix_tree_tfidf, annot=True, fmt='g', cmap=rdgy_palette)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
#---------------------------------------------------------------------------------------------------------------------------
```


```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Classification tree  ~ for embedding approach
#---------------------------------------------------------------------------------------------------------------------------
tree_params = {'max_depth': [3, 5, 10], 'min_samples_split': [2, 5, 10]}
# hyperparameter gridsearch 
tree_clf_emb = GridSearchCV(DecisionTreeClassifier(), tree_params, cv=5) 
tree_clf_emb.fit(X_train_emb, y_train)

# predictions and evaluation
y_pred_tree_emb = tree_clf_emb.predict(X_test_emb)
print("Best parameters:", tree_clf_emb.best_params_)
print("\nClassification Report (Classification Tree - embedding):")
print(classification_report(y_test, y_pred_tree_emb))

# confusion matrix
conf_matrix_tree_emb = confusion_matrix(y_test, y_pred_tree_emb)
sns.heatmap(conf_matrix_tree_emb, annot=True, fmt='g', cmap=rdgy_palette)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
#---------------------------------------------------------------------------------------------------------------------------
```



```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Feed-forward neural network ~ for BoW approach
#---------------------------------------------------------------------------------------------------------------------------
fnn_params = {'hidden_layer_sizes': [(100,), (100, 50)], 'activation': ['relu', 'tanh']}
# hyperparameter gridsearch 
fnn_clf_bow = GridSearchCV(MLPClassifier(max_iter=1000), fnn_params, cv=5)
fnn_clf_bow.fit(X_train_bow, y_train)

# Predictions and Evaluation
y_pred_fnn_bow = fnn_clf_bow.predict(X_test_bow)
print("Best parameters:", fnn_clf_bow.best_params_)
print("\nClassification Report (Feed-forward NN - BoW):")
print(classification_report(y_test, y_pred_fnn_bow))

# Confusion Matrix
conf_matrix_fnn_bow = confusion_matrix(y_test, y_pred_fnn_bow)
sns.heatmap(conf_matrix_fnn_bow, annot=True, fmt='g',cmap=rdgy_palette)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
#---------------------------------------------------------------------------------------------------------------------------
```


```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Feed-forward neural network ~ for tf-idf approach
#---------------------------------------------------------------------------------------------------------------------------
fnn_params = {'hidden_layer_sizes': [(100,), (100, 50)], 'activation': ['relu', 'tanh']}
# hyperparameter gridsearch 
fnn_clf_tfidf = GridSearchCV(MLPClassifier(max_iter=1000), fnn_params, cv=5)
fnn_clf_tfidf.fit(X_train_tfidf, y_train)

# Predictions and Evaluation
y_pred_fnn_tfidf = fnn_clf_tfidf.predict(X_test_tfidf)
print("Best parameters:", fnn_clf_tfidf.best_params_)
print("\nClassification Report (Feed-forward NN - tf-idf):")
print(classification_report(y_test, y_pred_fnn_tfidf))

# Confusion Matrix
conf_matrix_fnn_tfidf = confusion_matrix(y_test, y_pred_fnn_tfidf)
sns.heatmap(conf_matrix_fnn_tfidf, annot=True, fmt='g', cmap=rdgy_palette)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
#---------------------------------------------------------------------------------------------------------------------------
```

```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Feed-forward neural network ~ for embedding approach
#---------------------------------------------------------------------------------------------------------------------------
fnn_params = {'hidden_layer_sizes': [(100,), (100, 50)], 'activation': ['relu', 'tanh']}
# hyperparameter gridsearch 
fnn_clf_emb = GridSearchCV(MLPClassifier(max_iter=1000), fnn_params, cv=5)
fnn_clf_emb.fit(X_train_emb, y_train)

# Predictions and Evaluation
y_pred_fnn_emb = fnn_clf_emb.predict(X_test_emb)
print("Best parameters:", fnn_clf_emb.best_params_)
print("\nClassification Report (Feed-forward NN - embedding):")
print(classification_report(y_test, y_pred_fnn_emb))

# Confusion Matrix
conf_matrix_fnn_emb = confusion_matrix(y_test, y_pred_fnn_emb)
sns.heatmap(conf_matrix_fnn_emb, annot=True, fmt='g', cmap=rdgy_palette)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()


#---------------------------------------------------------------------------------------------------------------------------
```



```{python}
#---------------------------------------------------------------------------------------------------------------------------
#  Recurrent neural network ~ defined for all approaches
#---------------------------------------------------------------------------------------------------------------------------
def build_rnn_model(hp):
    model = Sequential()
    model.add(Embedding(input_dim=150, output_dim=hp.Int('embedding_output', min_value=32, max_value=128, step=32), input_length=150))
    model.add(SpatialDropout1D(hp.Float('spatial_dropout', min_value=0.0, max_value=0.5, step=0.1)))
    model.add(LSTM(units=hp.Int('lstm_units', min_value=50, max_value=150, step=25), dropout=hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1), recurrent_dropout=hp.Float('recurrent_dropout', min_value=0.0, max_value=0.5, step=0.1)))
    model.add(Dense(units=len(y_train_nn[0]), activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model

```


```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Recurrent neural network ~ for BoW approach
#---------------------------------------------------------------------------------------------------------------------------
# prepare features 
y_train_nn = to_categorical(pd.factorize(y_train)[0])
y_val_nn = to_categorical(pd.factorize(y_val)[0])
y_test_nn = to_categorical(pd.factorize(y_test)[0])

# RNN Tuner
rnn_tuner_bow = RandomSearch(build_rnn_model, objective='val_accuracy', max_trials=5, executions_per_trial=3, directory='rnn_tuner', project_name='RNNHyperparamTuning')
rnn_tuner_bow.search(X_train_bow, y_train_nn, epochs=10, validation_data=(X_val_bow, y_val_nn))

# Fit the best model
best_hps_rnn_bow = rnn_tuner_bow.get_best_hyperparameters(num_trials=1)[0]
best_rnn_model_bow = build_rnn_model(best_hps_rnn_bow)
history_rnn_bow = best_rnn_model_bow.fit(X_train_bow, y_train_nn, epochs=10, validation_data=(X_val_bow, y_val_nn))

# Predictions and Evaluation
y_pred_rnn_bow = best_rnn_model_bow.predict(X_test_bow)
y_pred_rnn_labels_bow = np.argmax(y_pred_rnn_bow, axis=1)
y_test_label_bow = np.argmax(y_test_nn, axis=1)

print("\nClassification Report (RNN - BoW):")
print(classification_report(y_test_label_bow, y_pred_rnn_labels_bow))

# Confusion Matrix
conf_matrix_rnn_bow = confusion_matrix(y_test_label_bow, y_pred_rnn_labels_bow)
sns.heatmap(conf_matrix_rnn_bow, annot=True, fmt='g', cmap=rdgy_palette)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Plotting accuracy curves
plt.plot(history_rnn_bow.history['accuracy'], color=rdgy_palette[9])
plt.plot(history_rnn_bow.history['val_accuracy'], color=rdgy_palette[0])
plt.title('RNN Model Accuracy - BoW')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()
#---------------------------------------------------------------------------------------------------------------------------
```



```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Recurrent neural network ~ for tf-idf approach
#---------------------------------------------------------------------------------------------------------------------------
# prepare features 
y_train_nn = to_categorical(pd.factorize(y_train)[0])
y_val_nn = to_categorical(pd.factorize(y_val)[0])
y_test_nn = to_categorical(pd.factorize(y_test)[0])

# RNN Tuner
rnn_tuner_tfidf = RandomSearch(build_rnn_model, objective='val_accuracy', max_trials=5, executions_per_trial=3, directory='rnn_tuner', project_name='RNNHyperparamTuning')
rnn_tuner_tfidf.search(X_train_tfidf, y_train_nn, epochs=10, validation_data=(X_val_tfidf, y_val_nn))

# Fit the best model
best_hps_rnn_tfidf = rnn_tuner_tfidf.get_best_hyperparameters(num_trials=1)[0]
best_rnn_model_tfidf = build_rnn_model(best_hps_rnn_tfidf)
history_rnn_tfidf = best_rnn_model_tfidf.fit(X_train_tfidf, y_train_nn, epochs=10, validation_data=(X_val_tfidf, y_val_nn))

# Predictions and Evaluation
y_pred_rnn_tfidf = best_rnn_model_tfidf.predict(X_test_tfidf)
y_pred_rnn_labels_tfidf = np.argmax(y_pred_rnn_tfidf, axis=1)
y_test_label_tfidf = np.argmax(y_test_nn, axis=1)

print("\nClassification Report (RNN - tf-idf):")
print(classification_report(y_test_label_tfidf, y_pred_rnn_labels_tfidf))

# Confusion Matrix
conf_matrix_rnn_tfidf = confusion_matrix(y_test_label_tfidf, y_pred_rnn_labels_tfidf)
sns.heatmap(conf_matrix_rnn_tfidf, annot=True, fmt='g')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Plotting accuracy curves
plt.plot(history_rnn_tfidf.history['accuracy'], color=rdgy_palette[9])
plt.plot(history_rnn_tfidf.history['val_accuracy'], color=rdgy_palette[0])
plt.title('RNN Model Accuracy - tf-idf')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()
#---------------------------------------------------------------------------------------------------------------------------
```


```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Recurrent neural network ~ for embedding approach
#---------------------------------------------------------------------------------------------------------------------------
# prepare features 
y_train_nn = to_categorical(pd.factorize(y_train)[0])
y_val_nn = to_categorical(pd.factorize(y_val)[0])
y_test_nn = to_categorical(pd.factorize(y_test)[0])

# RNN Tuner
rnn_tuner_emb = RandomSearch(build_rnn_model, objective='val_accuracy', max_trials=5, executions_per_trial=3, directory='rnn_tuner', project_name='RNNHyperparamTuning')
rnn_tuner_emb.search(X_train_emb, y_train_nn, epochs=10, validation_data=(X_val_emb, y_val_nn))

# Fit the best model
best_hps_rnn_emb = rnn_tuner_emb.get_best_hyperparameters(num_trials=1)[0]
best_rnn_model_emb = build_rnn_model(best_hps_rnn_emb)
history_rnn_emb = best_rnn_model_emb.fit(X_train_emb, y_train_nn, epochs=10, validation_data=(X_val_emb, y_val_nn))

# Predictions and Evaluation
y_pred_rnn_emb = best_rnn_model_emb.predict(X_test_emb)
y_pred_rnn_labels_emb = np.argmax(y_pred_rnn_emb, axis=1)
y_test_label_emb = np.argmax(y_test_nn, axis=1)

print("\nClassification Report (RNN - embedding):")
print(classification_report(y_test_label_emb, y_pred_rnn_labels_emb))

# Confusion Matrix
conf_matrix_rnn_emb = confusion_matrix(y_test_label_emb, y_pred_rnn_labels_emb)
sns.heatmap(conf_matrix_rnn_emb, annot=True, fmt='g')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Plotting accuracy curves
plt.plot(history_rnn_emb.history['accuracy'], color=rdgy_palette[9])
plt.plot(history_rnn_emb.history['val_accuracy'], color=rdgy_palette[0])
plt.title('RNN Model Accuracy - embedding')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()
#---------------------------------------------------------------------------------------------------------------------------
```


```{python}
#---------------------------------------------------------------------------------------------------------------------------
#  Convolutional neural network ~ defined for all approaches
#---------------------------------------------------------------------------------------------------------------------------
# define CNN 
def build_cnn_model(hp):
    model = Sequential()
    model.add(Input(shape=(150,)))  # Adjust to the shape of your Word2Vec embeddings
    model.add(Reshape((150, 1)))  # Add a reshape layer to make it compatible with Conv1D
    model.add(Conv1D(filters=hp.Int('filters', min_value=64, max_value=256, step=64), 
                     kernel_size=hp.Choice('kernel_size', values=[3, 5, 7]), 
                     activation='relu'))
    model.add(GlobalMaxPooling1D())
    model.add(Dropout(rate=hp.Float('dropout', min_value=0, max_value=0.5, step=0.1)))
    model.add(Dense(units=len(y_train_nn[0]), activation='softmax'))
    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
    return model


```


```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Convolutional neural network ~ for BoW approach
#---------------------------------------------------------------------------------------------------------------------------
# prepare features 
y_train_nn = to_categorical(pd.factorize(y_train)[0])
y_val_nn = to_categorical(pd.factorize(y_val)[0])
y_test_nn = to_categorical(pd.factorize(y_test)[0])


# CNN tuner
cnn_tuner_bow = RandomSearch(build_cnn_model, objective='val_accuracy', max_trials=5, executions_per_trial=3, directory='cnn_tuner', project_name='CNNHyperparamTuning')
cnn_tuner_bow.search(X_train_bow, y_train_nn, epochs=10, validation_data=(X_val_bow, y_val_nn))

# fit best CNN model 
best_hps_cnn_bow = cnn_tuner_bow.get_best_hyperparameters(num_trials=1)[0]
best_cnn_model_bow = build_cnn_model(best_hps_cnn_bow)
history_cnn_bow = best_cnn_model_bow.fit(X_train_bow, y_train_nn, epochs=10, validation_data=(X_val_bow, y_val_nn))

# predictions and evaluation
y_pred_cnn_bow = best_cnn_model_bow.predict(X_test_bow)
y_pred_cnn_labels_bow = np.argmax(y_pred_cnn_bow, axis=1)
y_test_labels_bow = np.argmax(y_test_nn, axis=1)

print("\nClassification Report (CNN - BoW):")
print(classification_report(y_test_labels_bow, y_pred_cnn_labels_bow))

# confusion matrix
conf_matrix_cnn_bow = confusion_matrix(y_test_labels_bow, y_pred_cnn_labels_bow)
sns.heatmap(conf_matrix_cnn_bow, annot=True, fmt='g')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# plotting accuracy curves
plt.plot(history_cnn_bow.history['accuracy'], color=rdgy_palette[9])
plt.plot(history_cnn_bow.history['val_accuracy'], color=rdgy_palette[0])
plt.title('CNN Model Accuracy - BoW')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()
#---------------------------------------------------------------------------------------------------------------------------
```


```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Convolutional neural network ~ for tf-idf approach
#---------------------------------------------------------------------------------------------------------------------------
# prepare features 
y_train_nn = to_categorical(pd.factorize(y_train)[0])
y_val_nn = to_categorical(pd.factorize(y_val)[0])
y_test_nn = to_categorical(pd.factorize(y_test)[0])

# CNN tuner
cnn_tuner_tfidf = RandomSearch(build_cnn_model, objective='val_accuracy', max_trials=5, executions_per_trial=3, directory='cnn_tuner', project_name='CNNHyperparamTuning')
cnn_tuner_tfidf.search(X_train_tfidf, y_train_nn, epochs=10, validation_data=(X_val_tfidf, y_val_nn))

# fit best CNN model 
best_hps_cnn_tfidf = cnn_tuner_tfidf.get_best_hyperparameters(num_trials=1)[0]
best_cnn_model_tfidf = build_cnn_model(best_hps_cnn_tfidf)
history_cnn_tfidf = best_cnn_model_tfidf.fit(X_train_tfidf, y_train_nn, epochs=10, validation_data=(X_val_tfidf, y_val_nn))

# predictions and evaluation
y_pred_cnn_tfidf = best_cnn_model_tfidf.predict(X_test_tfidf)
y_pred_cnn_labels_tfidf = np.argmax(y_pred_cnn_tfidf, axis=1)
y_test_labels_tfidf = np.argmax(y_test_nn, axis=1)

print("\nClassification Report (CNN - tf-idf):")
print(classification_report(y_test_labels_tfidf, y_pred_cnn_labels_tfidf))

# confusion matrix
conf_matrix_cnn_tfidf = confusion_matrix(y_test_labels_tfidf, y_pred_cnn_labels_tfidf)
sns.heatmap(conf_matrix_cnn_tfidf, annot=True, fmt='g')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# plotting accuracy curves
plt.plot(history_cnn_tfidf.history['accuracy'], color=rdgy_palette[9])
plt.plot(history_cnn_tfidf.history['val_accuracy'], color=rdgy_palette[0])
plt.title('CNN Model Accuracy - tf-idf')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()
#---------------------------------------------------------------------------------------------------------------------------
```


```{python}
#---------------------------------------------------------------------------------------------------------------------------
# Convolutional neural network ~ for embedding approach
#---------------------------------------------------------------------------------------------------------------------------
# prepare features 
y_train_nn = to_categorical(pd.factorize(y_train)[0])
y_val_nn = to_categorical(pd.factorize(y_val)[0])
y_test_nn = to_categorical(pd.factorize(y_test)[0])

# CNN tuner
cnn_tuner_emb = RandomSearch(build_cnn_model, objective='val_accuracy', max_trials=10, executions_per_trial=5, directory='cnn_tuner', project_name='CNNHyperparamTuning')
cnn_tuner_emb.search(X_train_emb, y_train_nn, epochs=10, validation_data=(X_val_emb, y_val_nn))

# fit best CNN model 
best_hps_cnn_emb = cnn_tuner_emb.get_best_hyperparameters(num_trials=1)[0]
best_cnn_model_emb = build_cnn_model(best_hps_cnn_emb)
history_cnn_emb = best_cnn_model_emb.fit(X_train_emb, y_train_nn, epochs=10, validation_data=(X_val_emb, y_val_nn))

# fit best CNN model 
best_hps_cnn_tfidf = cnn_tuner_tfidf.get_best_hyperparameters(num_trials=1)[0]
best_cnn_model_tfidf = build_cnn_model(best_hps_cnn_tfidf)
history_cnn_tfidf = best_cnn_model_tfidf.fit(X_train_tfidf, y_train_nn, epochs=10, validation_data=(X_val_tfidf, y_val_nn))

# predictions and evaluation
y_pred_cnn_emb = best_cnn_model_emb.predict(X_test_emb)
y_pred_cnn_labels_emb = np.argmax(y_pred_cnn_emb, axis=1)
y_test_labels_emb = np.argmax(y_test_nn, axis=1)

print("\nClassification Report (CNN - embedding):")
print(classification_report(y_test_labels_emb, y_pred_cnn_labels_emb))

# confusion matrix
conf_matrix_cnn_emb = confusion_matrix(y_test_labels_emb, y_pred_cnn_labels_emb)
sns.heatmap(conf_matrix_cnn_emb, annot=True, fmt='g')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# plotting accuracy curves
plt.plot(history_cnn_emb.history['accuracy'], color=rdgy_palette[9])
plt.plot(history_cnn_emb.history['val_accuracy'], color=rdgy_palette[0])
plt.title('CNN Model Accuracy - embedding')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()
```




<h4> Discussion and Conclusion </h4> 
<hr> 


<h4> References </h4> 
<hr> 

</div> 





